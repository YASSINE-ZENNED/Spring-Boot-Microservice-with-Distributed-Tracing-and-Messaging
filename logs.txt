* 
* ==> Audit <==
* |---------|--------------------------|----------|-----------|---------|---------------------|---------------------|
| Command |           Args           | Profile  |   User    | Version |     Start Time      |      End Time       |
|---------|--------------------------|----------|-----------|---------|---------------------|---------------------|
| start   | --memory=4g              | minikube | MRP\yassi | v1.32.0 | 16 Feb 24 15:58 CET | 16 Feb 24 16:06 CET |
| ip      |                          | minikube | MRP\yassi | v1.32.0 | 16 Feb 24 16:42 CET | 16 Feb 24 16:42 CET |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 19 Feb 24 16:52 CET | 19 Feb 24 16:52 CET |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 19 Feb 24 17:51 CET | 19 Feb 24 17:52 CET |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 20 Feb 24 16:43 CET | 20 Feb 24 16:43 CET |
| ssh     |                          | minikube | MRP\yassi | v1.32.0 | 20 Feb 24 17:19 CET | 20 Feb 24 17:20 CET |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 14:37 CET | 22 Feb 24 14:38 CET |
| service | --url rabbitmq           | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 14:52 CET | 22 Feb 24 14:53 CET |
| service | --url zipkin             | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 14:53 CET |                     |
| service | --url rabbitmq           | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 14:53 CET |                     |
| tunnel  |                          | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 19:06 CET |                     |
| service | --url zipkin             | minikube | MRP\yassi | v1.32.0 | 22 Feb 24 19:14 CET |                     |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 09 Sep 24 14:49 CET | 09 Sep 24 14:50 CET |
| tunnel  |                          | minikube | MRP\yassi | v1.32.0 | 09 Sep 24 15:18 CET |                     |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 01 Dec 24 14:03 CET | 01 Dec 24 14:04 CET |
| tunnel  |                          | minikube | MRP\yassi | v1.32.0 | 01 Dec 24 14:08 CET |                     |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 02 Dec 24 10:29 CET | 02 Dec 24 10:30 CET |
| tunnel  |                          | minikube | MRP\yassi | v1.32.0 | 02 Dec 24 10:30 CET |                     |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 08 Dec 24 15:56 CET |                     |
| start   |                          | minikube | MRP\yassi | v1.32.0 | 08 Dec 24 15:59 CET | 08 Dec 24 16:00 CET |
| tunnel  |                          | minikube | MRP\yassi | v1.32.0 | 08 Dec 24 16:00 CET |                     |
| service | prometheus -n monitoring | minikube | MRP\yassi | v1.32.0 | 08 Dec 24 16:18 CET |                     |
|---------|--------------------------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/12/08 15:59:33
Running on machine: MrP
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1208 15:59:33.681277   21612 out.go:296] Setting OutFile to fd 92 ...
I1208 15:59:33.681277   21612 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1208 15:59:33.681277   21612 out.go:309] Setting ErrFile to fd 100...
I1208 15:59:33.681277   21612 out.go:343] TERM=,COLORTERM=, which probably does not support color
W1208 15:59:33.694309   21612 root.go:314] Error reading config file at C:\Users\yassi\.minikube\config\config.json: open C:\Users\yassi\.minikube\config\config.json: The system cannot find the file specified.
I1208 15:59:33.698508   21612 out.go:303] Setting JSON to false
I1208 15:59:33.703183   21612 start.go:128] hostinfo: {"hostname":"MrP","uptime":152652,"bootTime":1733517321,"procs":292,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4460 Build 22631.4460","kernelVersion":"10.0.22631.4460 Build 22631.4460","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"678b1d61-8235-4192-8784-e0e0a147be75"}
W1208 15:59:33.703183   21612 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1208 15:59:33.704217   21612 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Home Single Language 10.0.22631.4460 Build 22631.4460
I1208 15:59:33.705255   21612 notify.go:220] Checking for updates...
I1208 15:59:33.705773   21612 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1208 15:59:33.705773   21612 driver.go:378] Setting default libvirt URI to qemu:///system
I1208 15:59:33.844549   21612 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.23.0 (120376)
I1208 15:59:33.846130   21612 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1208 15:59:34.299207   21612 info.go:266] docker info: {ID:7bce5d36-7b0d-4d36-abcf-b832158e865c Containers:35 ContainersRunning:10 ContainersPaused:0 ContainersStopped:25 Images:38 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:156 OomKillDisable:true NGoroutines:170 SystemTime:2024-12-08 14:59:34.254874159 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:14582726656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.7] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.24.1]] Warnings:<nil>}}
I1208 15:59:34.301310   21612 out.go:177] * Using the docker driver based on existing profile
I1208 15:59:34.306018   21612 start.go:298] selected driver: docker
I1208 15:59:34.306018   21612 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\yassi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1208 15:59:34.306018   21612 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1208 15:59:34.309690   21612 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1208 15:59:34.546872   21612 info.go:266] docker info: {ID:7bce5d36-7b0d-4d36-abcf-b832158e865c Containers:35 ContainersRunning:10 ContainersPaused:0 ContainersStopped:25 Images:38 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:156 OomKillDisable:true NGoroutines:170 SystemTime:2024-12-08 14:59:34.502549785 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:14582726656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.7] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.24.1]] Warnings:<nil>}}
I1208 15:59:34.632148   21612 cni.go:84] Creating CNI manager for ""
I1208 15:59:34.632148   21612 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1208 15:59:34.632148   21612 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\yassi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1208 15:59:34.633175   21612 out.go:177] * Starting control plane node minikube in cluster minikube
I1208 15:59:34.633694   21612 cache.go:121] Beginning downloading kic base image for docker with docker
I1208 15:59:34.634212   21612 out.go:177] * Pulling base image ...
I1208 15:59:34.635250   21612 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1208 15:59:34.635250   21612 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1208 15:59:34.637324   21612 preload.go:148] Found local preload: C:\Users\yassi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1208 15:59:34.637324   21612 cache.go:56] Caching tarball of preloaded images
I1208 15:59:34.637324   21612 preload.go:174] Found C:\Users\yassi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1208 15:59:34.637324   21612 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1208 15:59:34.637845   21612 profile.go:148] Saving config to C:\Users\yassi\.minikube\profiles\minikube\config.json ...
I1208 15:59:34.749027   21612 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I1208 15:59:34.749027   21612 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I1208 15:59:34.749027   21612 cache.go:194] Successfully downloaded all kic artifacts
I1208 15:59:34.749027   21612 start.go:365] acquiring machines lock for minikube: {Name:mk9cb97c853f0fd2b29e5568b9faa4923a27a9c8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1208 15:59:34.749532   21612 start.go:369] acquired machines lock for "minikube" in 0s
I1208 15:59:34.749532   21612 start.go:96] Skipping create...Using existing machine configuration
I1208 15:59:34.749532   21612 fix.go:54] fixHost starting: 
I1208 15:59:34.752652   21612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1208 15:59:34.857524   21612 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1208 15:59:34.857524   21612 fix.go:128] unexpected machine state, will restart: <nil>
I1208 15:59:34.859099   21612 out.go:177] * Restarting existing docker container for "minikube" ...
I1208 15:59:34.862290   21612 cli_runner.go:164] Run: docker start minikube
I1208 15:59:35.280754   21612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1208 15:59:35.388074   21612 kic.go:430] container "minikube" state is running.
I1208 15:59:35.391838   21612 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1208 15:59:35.495678   21612 profile.go:148] Saving config to C:\Users\yassi\.minikube\profiles\minikube\config.json ...
I1208 15:59:35.496706   21612 machine.go:88] provisioning docker machine ...
I1208 15:59:35.496706   21612 ubuntu.go:169] provisioning hostname "minikube"
I1208 15:59:35.498779   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:35.604993   21612 main.go:141] libmachine: Using SSH client type: native
I1208 15:59:35.612301   21612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf647e0] 0xf67320 <nil>  [] 0s} 127.0.0.1 57222 <nil> <nil>}
I1208 15:59:35.612301   21612 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1208 15:59:35.613872   21612 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1208 15:59:38.786026   21612 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1208 15:59:38.788095   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:38.887768   21612 main.go:141] libmachine: Using SSH client type: native
I1208 15:59:38.888283   21612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf647e0] 0xf67320 <nil>  [] 0s} 127.0.0.1 57222 <nil> <nil>}
I1208 15:59:38.888283   21612 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1208 15:59:39.005839   21612 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1208 15:59:39.005839   21612 ubuntu.go:175] set auth options {CertDir:C:\Users\yassi\.minikube CaCertPath:C:\Users\yassi\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\yassi\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\yassi\.minikube\machines\server.pem ServerKeyPath:C:\Users\yassi\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\yassi\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\yassi\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\yassi\.minikube}
I1208 15:59:39.005839   21612 ubuntu.go:177] setting up certificates
I1208 15:59:39.005839   21612 provision.go:83] configureAuth start
I1208 15:59:39.007974   21612 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1208 15:59:39.119182   21612 provision.go:138] copyHostCerts
I1208 15:59:39.126993   21612 exec_runner.go:144] found C:\Users\yassi\.minikube/ca.pem, removing ...
I1208 15:59:39.126993   21612 exec_runner.go:203] rm: C:\Users\yassi\.minikube\ca.pem
I1208 15:59:39.126993   21612 exec_runner.go:151] cp: C:\Users\yassi\.minikube\certs\ca.pem --> C:\Users\yassi\.minikube/ca.pem (1074 bytes)
I1208 15:59:39.135936   21612 exec_runner.go:144] found C:\Users\yassi\.minikube/cert.pem, removing ...
I1208 15:59:39.135936   21612 exec_runner.go:203] rm: C:\Users\yassi\.minikube\cert.pem
I1208 15:59:39.135936   21612 exec_runner.go:151] cp: C:\Users\yassi\.minikube\certs\cert.pem --> C:\Users\yassi\.minikube/cert.pem (1119 bytes)
I1208 15:59:39.143181   21612 exec_runner.go:144] found C:\Users\yassi\.minikube/key.pem, removing ...
I1208 15:59:39.143181   21612 exec_runner.go:203] rm: C:\Users\yassi\.minikube\key.pem
I1208 15:59:39.143181   21612 exec_runner.go:151] cp: C:\Users\yassi\.minikube\certs\key.pem --> C:\Users\yassi\.minikube/key.pem (1675 bytes)
I1208 15:59:39.143687   21612 provision.go:112] generating server cert: C:\Users\yassi\.minikube\machines\server.pem ca-key=C:\Users\yassi\.minikube\certs\ca.pem private-key=C:\Users\yassi\.minikube\certs\ca-key.pem org=yassi.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1208 15:59:39.276865   21612 provision.go:172] copyRemoteCerts
I1208 15:59:39.284809   21612 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1208 15:59:39.286789   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:39.385173   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 15:59:39.481178   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1208 15:59:39.508346   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I1208 15:59:39.529023   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1208 15:59:39.550170   21612 provision.go:86] duration metric: configureAuth took 544.3311ms
I1208 15:59:39.550170   21612 ubuntu.go:193] setting minikube options for container-runtime
I1208 15:59:39.550694   21612 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1208 15:59:39.552268   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:39.649722   21612 main.go:141] libmachine: Using SSH client type: native
I1208 15:59:39.650246   21612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf647e0] 0xf67320 <nil>  [] 0s} 127.0.0.1 57222 <nil> <nil>}
I1208 15:59:39.650246   21612 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1208 15:59:39.783876   21612 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1208 15:59:39.783876   21612 ubuntu.go:71] root file system type: overlay
I1208 15:59:39.783876   21612 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1208 15:59:39.786000   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:39.881758   21612 main.go:141] libmachine: Using SSH client type: native
I1208 15:59:39.882278   21612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf647e0] 0xf67320 <nil>  [] 0s} 127.0.0.1 57222 <nil> <nil>}
I1208 15:59:39.882278   21612 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1208 15:59:40.007804   21612 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1208 15:59:40.009375   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:40.114851   21612 main.go:141] libmachine: Using SSH client type: native
I1208 15:59:40.115378   21612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf647e0] 0xf67320 <nil>  [] 0s} 127.0.0.1 57222 <nil> <nil>}
I1208 15:59:40.115378   21612 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1208 15:59:40.255801   21612 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1208 15:59:40.255801   21612 machine.go:91] provisioned docker machine in 4.7590944s
I1208 15:59:40.255801   21612 start.go:300] post-start starting for "minikube" (driver="docker")
I1208 15:59:40.255801   21612 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1208 15:59:40.265251   21612 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1208 15:59:40.267347   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:40.377687   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 15:59:40.481494   21612 ssh_runner.go:195] Run: cat /etc/os-release
I1208 15:59:40.484891   21612 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1208 15:59:40.484891   21612 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1208 15:59:40.484891   21612 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1208 15:59:40.484891   21612 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1208 15:59:40.484891   21612 filesync.go:126] Scanning C:\Users\yassi\.minikube\addons for local assets ...
I1208 15:59:40.485411   21612 filesync.go:126] Scanning C:\Users\yassi\.minikube\files for local assets ...
I1208 15:59:40.485411   21612 start.go:303] post-start completed in 229.6098ms
I1208 15:59:40.493268   21612 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1208 15:59:40.494836   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:40.593578   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 15:59:40.687609   21612 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1208 15:59:40.692353   21612 fix.go:56] fixHost completed within 5.9428206s
I1208 15:59:40.692353   21612 start.go:83] releasing machines lock for "minikube", held for 5.9428206s
I1208 15:59:40.693922   21612 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1208 15:59:40.796213   21612 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1208 15:59:40.798313   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:40.806207   21612 ssh_runner.go:195] Run: cat /version.json
I1208 15:59:40.807778   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 15:59:40.902327   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 15:59:40.911236   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 15:59:41.330894   21612 ssh_runner.go:195] Run: systemctl --version
I1208 15:59:41.347306   21612 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1208 15:59:41.360164   21612 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1208 15:59:41.370095   21612 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1208 15:59:41.378380   21612 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1208 15:59:41.387259   21612 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1208 15:59:41.387259   21612 start.go:472] detecting cgroup driver to use...
I1208 15:59:41.387259   21612 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1208 15:59:41.387259   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1208 15:59:41.410032   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1208 15:59:41.429695   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1208 15:59:41.439427   21612 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1208 15:59:41.448788   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1208 15:59:41.466620   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1208 15:59:41.483828   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1208 15:59:41.501223   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1208 15:59:41.518830   21612 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1208 15:59:41.535667   21612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1208 15:59:41.552669   21612 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1208 15:59:41.569730   21612 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1208 15:59:41.586738   21612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1208 15:59:41.689905   21612 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1208 15:59:41.798523   21612 start.go:472] detecting cgroup driver to use...
I1208 15:59:41.798523   21612 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1208 15:59:41.806778   21612 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1208 15:59:41.818991   21612 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1208 15:59:41.827500   21612 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1208 15:59:41.840826   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1208 15:59:41.864053   21612 ssh_runner.go:195] Run: which cri-dockerd
I1208 15:59:41.879121   21612 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1208 15:59:41.888967   21612 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1208 15:59:41.917394   21612 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1208 15:59:42.031650   21612 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1208 15:59:42.120615   21612 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1208 15:59:42.120615   21612 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1208 15:59:42.146711   21612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1208 15:59:42.252178   21612 ssh_runner.go:195] Run: sudo systemctl restart docker
I1208 15:59:42.638289   21612 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1208 15:59:42.760433   21612 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1208 15:59:42.880646   21612 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1208 15:59:42.981652   21612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1208 15:59:43.102521   21612 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1208 15:59:43.124459   21612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1208 15:59:43.222174   21612 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1208 15:59:43.438076   21612 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1208 15:59:43.446978   21612 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1208 15:59:43.451102   21612 start.go:540] Will wait 60s for crictl version
I1208 15:59:43.459309   21612 ssh_runner.go:195] Run: which crictl
I1208 15:59:43.470810   21612 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1208 15:59:43.602652   21612 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1208 15:59:43.604227   21612 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1208 15:59:43.692658   21612 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1208 15:59:43.716788   21612 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1208 15:59:43.718342   21612 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1208 15:59:43.898433   21612 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1208 15:59:43.908302   21612 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1208 15:59:43.912884   21612 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1208 15:59:43.925409   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1208 15:59:44.039353   21612 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1208 15:59:44.041418   21612 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1208 15:59:44.063473   21612 docker.go:671] Got preloaded images: -- stdout --
openzipkin/zipkin:latest
openzipkin/zipkin:<none>
postgres:latest
openzipkin/zipkin:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
rabbitmq:3.8
amigoscode/kubernetes:hello-world
gcr.io/k8s-minikube/storage-provisioner:v5
postgres:10.1-alpine
postgres:10.1
yassinezenned/fraud:<none>
yassinezenned/customer:<none>
yassinezenned/notification:latest
yassinezenned/notification:<none>
yassinezenned/fraud:<none>
yassinezenned/customer:<none>
yassinezenned/customer:<none>
yassinezenned/customer:latest
yassinezenned/fraud:latest

-- /stdout --
I1208 15:59:44.063473   21612 docker.go:601] Images already preloaded, skipping extraction
I1208 15:59:44.065559   21612 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1208 15:59:44.084665   21612 docker.go:671] Got preloaded images: -- stdout --
openzipkin/zipkin:latest
openzipkin/zipkin:<none>
postgres:latest
openzipkin/zipkin:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
rabbitmq:3.8
amigoscode/kubernetes:hello-world
gcr.io/k8s-minikube/storage-provisioner:v5
postgres:10.1-alpine
postgres:10.1
yassinezenned/fraud:<none>
yassinezenned/fraud:latest
yassinezenned/customer:<none>
yassinezenned/customer:latest
yassinezenned/notification:latest
yassinezenned/customer:<none>
yassinezenned/fraud:<none>
yassinezenned/notification:<none>
yassinezenned/customer:<none>

-- /stdout --
I1208 15:59:44.084665   21612 cache_images.go:84] Images are preloaded, skipping loading
I1208 15:59:44.086772   21612 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1208 15:59:44.269461   21612 cni.go:84] Creating CNI manager for ""
I1208 15:59:44.269461   21612 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1208 15:59:44.269461   21612 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1208 15:59:44.269461   21612 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1208 15:59:44.269980   21612 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1208 15:59:44.269980   21612 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1208 15:59:44.278386   21612 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1208 15:59:44.288151   21612 binaries.go:44] Found k8s binaries, skipping transfer
I1208 15:59:44.297138   21612 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1208 15:59:44.306003   21612 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1208 15:59:44.321356   21612 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1208 15:59:44.336437   21612 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1208 15:59:44.362325   21612 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1208 15:59:44.366057   21612 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1208 15:59:44.376587   21612 certs.go:56] Setting up C:\Users\yassi\.minikube\profiles\minikube for IP: 192.168.49.2
I1208 15:59:44.376587   21612 certs.go:190] acquiring lock for shared ca certs: {Name:mk691b4a88a7bfbae45076daabd1f575beaa05dc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1208 15:59:44.383867   21612 certs.go:199] skipping minikubeCA CA generation: C:\Users\yassi\.minikube\ca.key
I1208 15:59:44.394249   21612 certs.go:199] skipping proxyClientCA CA generation: C:\Users\yassi\.minikube\proxy-client-ca.key
I1208 15:59:44.394770   21612 certs.go:315] skipping minikube-user signed cert generation: C:\Users\yassi\.minikube\profiles\minikube\client.key
I1208 15:59:44.405695   21612 certs.go:315] skipping minikube signed cert generation: C:\Users\yassi\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1208 15:59:44.414251   21612 certs.go:315] skipping aggregator signed cert generation: C:\Users\yassi\.minikube\profiles\minikube\proxy-client.key
I1208 15:59:44.417381   21612 certs.go:437] found cert: C:\Users\yassi\.minikube\certs\C:\Users\yassi\.minikube\certs\ca-key.pem (1679 bytes)
I1208 15:59:44.417381   21612 certs.go:437] found cert: C:\Users\yassi\.minikube\certs\C:\Users\yassi\.minikube\certs\ca.pem (1074 bytes)
I1208 15:59:44.417381   21612 certs.go:437] found cert: C:\Users\yassi\.minikube\certs\C:\Users\yassi\.minikube\certs\cert.pem (1119 bytes)
I1208 15:59:44.417890   21612 certs.go:437] found cert: C:\Users\yassi\.minikube\certs\C:\Users\yassi\.minikube\certs\key.pem (1675 bytes)
I1208 15:59:44.418418   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1208 15:59:44.440678   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1208 15:59:44.462228   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1208 15:59:44.484491   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1208 15:59:44.506136   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1208 15:59:44.528413   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1208 15:59:44.549603   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1208 15:59:44.571621   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1208 15:59:44.593896   21612 ssh_runner.go:362] scp C:\Users\yassi\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1208 15:59:44.615415   21612 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1208 15:59:44.639452   21612 ssh_runner.go:195] Run: openssl version
I1208 15:59:44.656604   21612 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1208 15:59:44.675387   21612 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1208 15:59:44.679490   21612 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 16  2024 /usr/share/ca-certificates/minikubeCA.pem
I1208 15:59:44.687755   21612 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1208 15:59:44.702294   21612 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1208 15:59:44.719810   21612 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1208 15:59:44.732669   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1208 15:59:44.747788   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1208 15:59:44.762548   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1208 15:59:44.776906   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1208 15:59:44.791991   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1208 15:59:44.807281   21612 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1208 15:59:44.813364   21612 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\yassi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1208 15:59:44.815491   21612 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1208 15:59:44.840889   21612 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1208 15:59:44.849259   21612 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1208 15:59:44.849259   21612 kubeadm.go:636] restartCluster start
I1208 15:59:44.857535   21612 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1208 15:59:44.865638   21612 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1208 15:59:44.867199   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1208 15:59:44.971061   21612 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:61444"
I1208 15:59:44.971061   21612 kubeconfig.go:135] verify returned: got: 127.0.0.1:61444, want: 127.0.0.1:57226
I1208 15:59:44.971579   21612 lock.go:35] WriteFile acquiring C:\Users\yassi\.kube\config: {Name:mk86fb64ca449a0e0e1a5b13b3af31d422858c27 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1208 15:59:44.991051   21612 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1208 15:59:45.000980   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:45.009851   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:45.021718   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:45.021718   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:45.029496   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:45.039856   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:45.553443   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:45.564016   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:45.576097   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:46.043470   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:46.052951   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:46.064395   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:46.550606   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:46.560179   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:46.571298   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:47.054784   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:47.067322   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:47.078656   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:47.540785   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:47.549149   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:47.559850   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:48.043686   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:48.052586   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:48.065016   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:48.543316   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:48.553931   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:48.565607   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:49.042235   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:49.052360   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:49.064422   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:49.550498   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:49.572903   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:49.589493   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:50.046572   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:50.055748   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:50.068410   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:50.540867   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:50.550390   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:50.562564   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:51.040504   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:51.049951   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:51.060535   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:51.542158   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:51.551579   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:51.562369   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:52.040231   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:52.049184   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:52.059502   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:52.550803   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:52.560709   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:52.571766   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:53.043318   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:53.051690   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:53.061901   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:53.549463   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:53.558851   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:53.570753   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:54.040868   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:54.051276   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:54.063310   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:54.540399   21612 api_server.go:166] Checking apiserver status ...
I1208 15:59:54.554324   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1208 15:59:54.608268   21612 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1208 15:59:55.001908   21612 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1208 15:59:55.001908   21612 kubeadm.go:1128] stopping kube-system containers ...
I1208 15:59:55.003469   21612 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1208 15:59:55.041325   21612 docker.go:469] Stopping containers: [592126260fae 8e89ab0bbaf1 c42da27303c8 9906483a64a8 bcc80ebc48fc e25f19c57379 70ba04c1d36f 7b6138c1f074 11d9b5a8dcb8 dac6984c06d4 6280cf4e85ee 8d6de95066d9 34a26bb92c52 3b8980264957 196fb8e8d748 cc1f66de0e75 bb754af7d281 d8e5c4396570 48ad242a1fb1 e975c35e89e3 cf182deecd77 a7043b69f11f 694aa40f2bc9 9c5db09980a9 dd5d2eeea11e 72ed78a62807 55c07874c6df]
I1208 15:59:55.043435   21612 ssh_runner.go:195] Run: docker stop 592126260fae 8e89ab0bbaf1 c42da27303c8 9906483a64a8 bcc80ebc48fc e25f19c57379 70ba04c1d36f 7b6138c1f074 11d9b5a8dcb8 dac6984c06d4 6280cf4e85ee 8d6de95066d9 34a26bb92c52 3b8980264957 196fb8e8d748 cc1f66de0e75 bb754af7d281 d8e5c4396570 48ad242a1fb1 e975c35e89e3 cf182deecd77 a7043b69f11f 694aa40f2bc9 9c5db09980a9 dd5d2eeea11e 72ed78a62807 55c07874c6df
I1208 15:59:55.088814   21612 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1208 15:59:55.118475   21612 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1208 15:59:55.130749   21612 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Feb 16  2024 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Dec  2 09:30 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Feb 16  2024 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Dec  2 09:30 /etc/kubernetes/scheduler.conf

I1208 15:59:55.148138   21612 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1208 15:59:55.204622   21612 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1208 15:59:55.244413   21612 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1208 15:59:55.259337   21612 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1208 15:59:55.274665   21612 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1208 15:59:55.295439   21612 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1208 15:59:55.307078   21612 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1208 15:59:55.330451   21612 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1208 15:59:55.354082   21612 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1208 15:59:55.366219   21612 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1208 15:59:55.366219   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1208 15:59:55.552131   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1208 15:59:56.180499   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1208 15:59:56.354291   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1208 15:59:56.411063   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1208 15:59:56.469577   21612 api_server.go:52] waiting for apiserver process to appear ...
I1208 15:59:56.481532   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 15:59:56.508294   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 15:59:57.052317   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 15:59:57.541527   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 15:59:58.043392   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 15:59:58.072537   21612 api_server.go:72] duration metric: took 1.6029602s to wait for apiserver process to appear ...
I1208 15:59:58.072537   21612 api_server.go:88] waiting for apiserver healthz status ...
I1208 15:59:58.072537   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 15:59:58.074109   21612 api_server.go:269] stopped: https://127.0.0.1:57226/healthz: Get "https://127.0.0.1:57226/healthz": EOF
I1208 15:59:58.074109   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 15:59:58.077295   21612 api_server.go:269] stopped: https://127.0.0.1:57226/healthz: Get "https://127.0.0.1:57226/healthz": EOF
I1208 15:59:58.592007   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 15:59:58.594076   21612 api_server.go:269] stopped: https://127.0.0.1:57226/healthz: Get "https://127.0.0.1:57226/healthz": EOF
I1208 15:59:59.088623   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:00.645860   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1208 16:00:00.645860   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1208 16:00:00.645860   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:00.653078   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1208 16:00:00.653078   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1208 16:00:01.083818   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:01.090200   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1208 16:00:01.090200   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1208 16:00:01.583168   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:01.597018   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1208 16:00:01.597018   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1208 16:00:02.079318   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:02.087912   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1208 16:00:02.087912   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1208 16:00:02.579787   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:02.591027   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1208 16:00:02.591027   21612 api_server.go:103] status: https://127.0.0.1:57226/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1208 16:00:03.081750   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:03.090861   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 200:
ok
I1208 16:00:03.101346   21612 api_server.go:141] control plane version: v1.28.3
I1208 16:00:03.101346   21612 api_server.go:131] duration metric: took 5.0288094s to wait for apiserver health ...
I1208 16:00:03.101346   21612 cni.go:84] Creating CNI manager for ""
I1208 16:00:03.101346   21612 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1208 16:00:03.104061   21612 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I1208 16:00:03.120907   21612 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1208 16:00:03.184075   21612 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1208 16:00:03.208390   21612 system_pods.go:43] waiting for kube-system pods to appear ...
I1208 16:00:03.217635   21612 system_pods.go:59] 7 kube-system pods found
I1208 16:00:03.217635   21612 system_pods.go:61] "coredns-5dd5756b68-ggk2k" [e9beadb7-3671-4b92-a488-077b6caba1f6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1208 16:00:03.217635   21612 system_pods.go:61] "etcd-minikube" [c796a099-4777-470a-b68d-22321bbd967d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1208 16:00:03.218566   21612 system_pods.go:61] "kube-apiserver-minikube" [982ab678-cf53-454b-94a1-1e9293d1b86c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1208 16:00:03.218566   21612 system_pods.go:61] "kube-controller-manager-minikube" [36a2fa97-49f8-4a00-a82e-47b406fa78ea] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1208 16:00:03.218566   21612 system_pods.go:61] "kube-proxy-h64xt" [972fe334-c183-409c-b342-d225e96ecd5b] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1208 16:00:03.218566   21612 system_pods.go:61] "kube-scheduler-minikube" [17d4463b-9064-4a24-b366-a4e12a6b75f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1208 16:00:03.218566   21612 system_pods.go:61] "storage-provisioner" [17d3e133-6df2-4903-ab37-e10a5c5d7167] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1208 16:00:03.218566   21612 system_pods.go:74] duration metric: took 10.1754ms to wait for pod list to return data ...
I1208 16:00:03.218566   21612 node_conditions.go:102] verifying NodePressure condition ...
I1208 16:00:03.225663   21612 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1208 16:00:03.225663   21612 node_conditions.go:123] node cpu capacity is 12
I1208 16:00:03.225663   21612 node_conditions.go:105] duration metric: took 7.097ms to run NodePressure ...
I1208 16:00:03.225663   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1208 16:00:03.487492   21612 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1208 16:00:03.497045   21612 ops.go:34] apiserver oom_adj: -16
I1208 16:00:03.497045   21612 kubeadm.go:640] restartCluster took 18.647786s
I1208 16:00:03.497045   21612 kubeadm.go:406] StartCluster complete in 18.6836813s
I1208 16:00:03.497045   21612 settings.go:142] acquiring lock: {Name:mk129eaaf1eeeb7f233e7f6a42a3bf0bbc65d195 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1208 16:00:03.497045   21612 settings.go:150] Updating kubeconfig:  C:\Users\yassi\.kube\config
I1208 16:00:03.497566   21612 lock.go:35] WriteFile acquiring C:\Users\yassi\.kube\config: {Name:mk86fb64ca449a0e0e1a5b13b3af31d422858c27 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1208 16:00:03.498094   21612 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1208 16:00:03.498624   21612 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1208 16:00:03.498624   21612 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1208 16:00:03.498624   21612 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1208 16:00:03.498624   21612 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1208 16:00:03.498624   21612 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1208 16:00:03.498624   21612 addons.go:240] addon storage-provisioner should already be in state true
I1208 16:00:03.498624   21612 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1208 16:00:03.499014   21612 host.go:66] Checking if "minikube" exists ...
I1208 16:00:03.504752   21612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1208 16:00:03.505277   21612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1208 16:00:03.517499   21612 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1208 16:00:03.517499   21612 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1208 16:00:03.518585   21612 out.go:177] * Verifying Kubernetes components...
I1208 16:00:03.530630   21612 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1208 16:00:03.657732   21612 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1208 16:00:03.659253   21612 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1208 16:00:03.659253   21612 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1208 16:00:03.661385   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 16:00:03.673807   21612 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1208 16:00:03.673807   21612 addons.go:240] addon default-storageclass should already be in state true
I1208 16:00:03.673807   21612 host.go:66] Checking if "minikube" exists ...
I1208 16:00:03.679275   21612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1208 16:00:03.736797   21612 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1208 16:00:03.739933   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1208 16:00:03.804716   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 16:00:03.830050   21612 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1208 16:00:03.830050   21612 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1208 16:00:03.831637   21612 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1208 16:00:03.868915   21612 api_server.go:52] waiting for apiserver process to appear ...
I1208 16:00:03.878327   21612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1208 16:00:03.891789   21612 api_server.go:72] duration metric: took 374.2897ms to wait for apiserver process to appear ...
I1208 16:00:03.891789   21612 api_server.go:88] waiting for apiserver healthz status ...
I1208 16:00:03.891789   21612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57226/healthz ...
I1208 16:00:03.897541   21612 api_server.go:279] https://127.0.0.1:57226/healthz returned 200:
ok
I1208 16:00:03.899118   21612 api_server.go:141] control plane version: v1.28.3
I1208 16:00:03.899118   21612 api_server.go:131] duration metric: took 7.3294ms to wait for apiserver health ...
I1208 16:00:03.899118   21612 system_pods.go:43] waiting for kube-system pods to appear ...
I1208 16:00:03.903926   21612 system_pods.go:59] 7 kube-system pods found
I1208 16:00:03.903926   21612 system_pods.go:61] "coredns-5dd5756b68-ggk2k" [e9beadb7-3671-4b92-a488-077b6caba1f6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1208 16:00:03.903926   21612 system_pods.go:61] "etcd-minikube" [c796a099-4777-470a-b68d-22321bbd967d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1208 16:00:03.903926   21612 system_pods.go:61] "kube-apiserver-minikube" [982ab678-cf53-454b-94a1-1e9293d1b86c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1208 16:00:03.903926   21612 system_pods.go:61] "kube-controller-manager-minikube" [36a2fa97-49f8-4a00-a82e-47b406fa78ea] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1208 16:00:03.903926   21612 system_pods.go:61] "kube-proxy-h64xt" [972fe334-c183-409c-b342-d225e96ecd5b] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1208 16:00:03.903926   21612 system_pods.go:61] "kube-scheduler-minikube" [17d4463b-9064-4a24-b366-a4e12a6b75f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1208 16:00:03.903926   21612 system_pods.go:61] "storage-provisioner" [17d3e133-6df2-4903-ab37-e10a5c5d7167] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1208 16:00:03.903926   21612 system_pods.go:74] duration metric: took 4.8082ms to wait for pod list to return data ...
I1208 16:00:03.903926   21612 kubeadm.go:581] duration metric: took 386.4273ms to wait for : map[apiserver:true system_pods:true] ...
I1208 16:00:03.903926   21612 node_conditions.go:102] verifying NodePressure condition ...
I1208 16:00:03.907072   21612 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1208 16:00:03.907072   21612 node_conditions.go:123] node cpu capacity is 12
I1208 16:00:03.907592   21612 node_conditions.go:105] duration metric: took 3.6655ms to run NodePressure ...
I1208 16:00:03.907592   21612 start.go:228] waiting for startup goroutines ...
I1208 16:00:03.928639   21612 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1208 16:00:03.946747   21612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57222 SSHKeyPath:C:\Users\yassi\.minikube\machines\minikube\id_rsa Username:docker}
I1208 16:00:04.101203   21612 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1208 16:00:04.599592   21612 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I1208 16:00:04.601157   21612 addons.go:502] enable addons completed in 1.103063s: enabled=[storage-provisioner default-storageclass]
I1208 16:00:04.601157   21612 start.go:233] waiting for cluster config update ...
I1208 16:00:04.601157   21612 start.go:242] writing updated cluster config ...
I1208 16:00:04.610132   21612 ssh_runner.go:195] Run: rm -f paused
I1208 16:00:04.701847   21612 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I1208 16:00:04.702355   21612 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"customer-7d78dfc6b-gzg9h_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"607f363ed3848922551fec936c91367d9d53e3bc0e3d39efe366336835db1bf7\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9b1188ed1c8c20023604a83e1d792463bafb1439548cb10d063106450319505f\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c4d3bc962ef3fcbead12c84643881cbd369a1c2c8e6986e1829e23698dd1ef02\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"zipkin-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"69630afd72158b06584d304eca3938c8d217ae559bd981582ef8a1becef6fa58\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"zipkin-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7a7d11cb91c9335da8c3c0d67f9705bfce8a1ac8bb071bc7e0be82436a31c382\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"fraud-659ff85d7-dfxhz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"76df7974e9d1d4eaf33db650cc9b04451fa1707a14662bb68a8a4c1a7244aebe\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"fraud-659ff85d7-dfxhz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"caf8443c56c2f96e082c26b16c78ffa4f8e3b04463323bdef828fa3cf3f7d98e\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"notification-6675954bc7-fm24n_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6992af9f4d98855234fd6e953f6af075861bbc4857140dc9e37a91c23dd00079\""
Dec 08 14:59:56 minikube cri-dockerd[1246]: time="2024-12-08T14:59:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"notification-6675954bc7-fm24n_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"33ce07e8270d8a9057c52e4c3077776faf22c77c41c44e45afdc048bf323504c\""
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-ggk2k_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bcc80ebc48fcf7323756e849f909afc73c585cc21b4a5bcada8e81133e5a60e2\""
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-ggk2k_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d8e5c43965700c12b96dede560ff4b440f9bb38f4472475aa8945bcc81b5d074\""
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"55c07874c6df857a1fb4b86f1c3c868b961274661c989a1c624f30b9baf75466\". Proceed without further sandbox information."
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"72ed78a628079666783029e76a087d356d1cc5fd2c3b0640bb8aad2f53599fa8\". Proceed without further sandbox information."
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"dd5d2eeea11ee6c93ede4cf9237fc0ef9df340399e82eb40a8b6cf6f71003604\". Proceed without further sandbox information."
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"9c5db09980a9eac4e79275e0e6e935bef936bfb6c97e708458cbda515b0788b9\". Proceed without further sandbox information."
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/df9558804971048538ddf7645298258fca9450e7dc5ccf0307898ff48c7db170/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b5626c61fd4d08dedff31a16c48105a0e907f5e325ed927f8e8e46e4d7a8946b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8c81abb5191ab7153e382f45bd4fa4a60bfe91fd0ca7ea02800accbf87139445/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 14:59:57 minikube cri-dockerd[1246]: time="2024-12-08T14:59:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c6aee9c60d02ca68621618c8f0a1082c691dc58d5a2de940f85f42adf8fdbcb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"customer-7d78dfc6b-gzg9h_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9dc76da52313331a5dfee845e45b97f3349217407cc652a43e39d3a6e67b6df5\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"fraud-659ff85d7-dfxhz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"76df7974e9d1d4eaf33db650cc9b04451fa1707a14662bb68a8a4c1a7244aebe\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rabbitmq-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9b1188ed1c8c20023604a83e1d792463bafb1439548cb10d063106450319505f\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"notification-6675954bc7-fm24n_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6992af9f4d98855234fd6e953f6af075861bbc4857140dc9e37a91c23dd00079\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgres-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"48ed6601d0ea69826ec7fd82e7f64d55068ba6ad7a5f2ba692a47d68fd2817b8\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-ggk2k_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bcc80ebc48fcf7323756e849f909afc73c585cc21b4a5bcada8e81133e5a60e2\""
Dec 08 14:59:58 minikube cri-dockerd[1246]: time="2024-12-08T14:59:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"zipkin-0_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"69630afd72158b06584d304eca3938c8d217ae559bd981582ef8a1becef6fa58\""
Dec 08 15:00:00 minikube cri-dockerd[1246]: time="2024-12-08T15:00:00Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 08 15:00:01 minikube cri-dockerd[1246]: time="2024-12-08T15:00:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b4be2bc3cc3427d8476b23ecbb46fdd9d56f7dba5372573248f1513a369cca6f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 15:00:01 minikube cri-dockerd[1246]: time="2024-12-08T15:00:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a812d59f2a3084d4f6b79b875c6f2bc8e0a70d8d51950fdbae7e2ae8ab3c8b4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/162affe57ae84a382b820d3cad982758e23df7ee9d799191df70b6295f421056/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1110558e4e217163c201f1c79ebd28b38e8c00849951434b74c4a63264057bb5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/10caad3a2fb2e8ad63bc62503b23e7fa31e82d7e88418acf8a057f7800d1e5d0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f9dbfc6537ae8d68928c900b3d31f8a8378a6136b742334ffccf880c169a79d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a5a4ee8e1a2f4c9598fbb307b67356ec900d7d10705779e9f27b40d9acc37bcd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e83a0b853e3250e8e7c441866fa00cca49c1afe63f6963d27be3beeca2bc469/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:02 minikube cri-dockerd[1246]: time="2024-12-08T15:00:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d83adac6f86b9f168c72562dae699078d1fdd1eb4149bbae2f25b4b24ae100e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:00:05 minikube cri-dockerd[1246]: time="2024-12-08T15:00:05Z" level=info msg="Stop pulling image yassinezenned/fraud:latest: Status: Image is up to date for yassinezenned/fraud:latest"
Dec 08 15:00:13 minikube dockerd[986]: time="2024-12-08T15:00:13.126309485Z" level=info msg="ignoring event" container=f48d1ee9abdfc8d3d0545e8eb8f91103685e8126768660e44ecb6046d9881794 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 15:00:17 minikube cri-dockerd[1246]: time="2024-12-08T15:00:17Z" level=info msg="Pulling image openzipkin/zipkin:latest: 7daf755e9979: Downloading [======>                                            ]  3.734MB/30.08MB"
Dec 08 15:00:22 minikube dockerd[986]: time="2024-12-08T15:00:22.894313304Z" level=info msg="ignoring event" container=21962c721571920229b33758b1ca715de1d97a54202d2f7a19792ca75ef71390 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 15:00:27 minikube cri-dockerd[1246]: time="2024-12-08T15:00:27Z" level=info msg="Pulling image openzipkin/zipkin:latest: 3fb9a9492171: Downloading [==========>                                        ]  15.12MB/74.31MB"
Dec 08 15:00:37 minikube cri-dockerd[1246]: time="2024-12-08T15:00:37Z" level=info msg="Pulling image openzipkin/zipkin:latest: 7daf755e9979: Downloading [================================>                  ]  19.29MB/30.08MB"
Dec 08 15:00:47 minikube cri-dockerd[1246]: time="2024-12-08T15:00:47Z" level=info msg="Pulling image openzipkin/zipkin:latest: 7daf755e9979: Downloading [===============================================>   ]   28.6MB/30.08MB"
Dec 08 15:00:57 minikube cri-dockerd[1246]: time="2024-12-08T15:00:57Z" level=info msg="Pulling image openzipkin/zipkin:latest: 3fb9a9492171: Downloading [=========================================>         ]  61.47MB/74.31MB"
Dec 08 15:01:03 minikube cri-dockerd[1246]: time="2024-12-08T15:01:03Z" level=info msg="Stop pulling image openzipkin/zipkin:latest: Status: Downloaded newer image for openzipkin/zipkin:latest"
Dec 08 15:01:05 minikube cri-dockerd[1246]: time="2024-12-08T15:01:05Z" level=info msg="Stop pulling image yassinezenned/notification:latest: Status: Image is up to date for yassinezenned/notification:latest"
Dec 08 15:01:06 minikube cri-dockerd[1246]: time="2024-12-08T15:01:06Z" level=info msg="Stop pulling image yassinezenned/customer:latest: Status: Image is up to date for yassinezenned/customer:latest"
Dec 08 15:01:08 minikube cri-dockerd[1246]: time="2024-12-08T15:01:08Z" level=info msg="Stop pulling image rabbitmq:3.8: Status: Image is up to date for rabbitmq:3.8"
Dec 08 15:01:10 minikube cri-dockerd[1246]: time="2024-12-08T15:01:10Z" level=info msg="Stop pulling image yassinezenned/fraud:latest: Status: Image is up to date for yassinezenned/fraud:latest"
Dec 08 15:18:38 minikube cri-dockerd[1246]: time="2024-12-08T15:18:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f330a8d054b9db04eb5cb8f85619b21e1cae56f229145bc687691f1e2da62cfc/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:18:50 minikube cri-dockerd[1246]: time="2024-12-08T15:18:50Z" level=info msg="Pulling image prom/prometheus:latest: 6e463c39e3fe: Downloading [=======>                                           ]  8.596MB/54.08MB"
Dec 08 15:19:00 minikube cri-dockerd[1246]: time="2024-12-08T15:19:00Z" level=info msg="Pulling image prom/prometheus:latest: 9818abf4f966: Downloading [=================>                                 ]     21MB/60.16MB"
Dec 08 15:19:10 minikube cri-dockerd[1246]: time="2024-12-08T15:19:10Z" level=info msg="Pulling image prom/prometheus:latest: 6e463c39e3fe: Downloading [=============================>                     ]  31.71MB/54.08MB"
Dec 08 15:19:20 minikube cri-dockerd[1246]: time="2024-12-08T15:19:20Z" level=info msg="Pulling image prom/prometheus:latest: 9818abf4f966: Downloading [==================================>                ]  41.99MB/60.16MB"
Dec 08 15:19:30 minikube cri-dockerd[1246]: time="2024-12-08T15:19:30Z" level=info msg="Pulling image prom/prometheus:latest: 9818abf4f966: Downloading [============================================>      ]  53.29MB/60.16MB"
Dec 08 15:19:35 minikube cri-dockerd[1246]: time="2024-12-08T15:19:35Z" level=info msg="Stop pulling image prom/prometheus:latest: Status: Downloaded newer image for prom/prometheus:latest"
Dec 08 15:20:59 minikube cri-dockerd[1246]: time="2024-12-08T15:20:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7050bcb8693886157b2c9cd1297450ad17d5796050a969d3e007696b62a963ea/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 08 15:21:02 minikube cri-dockerd[1246]: time="2024-12-08T15:21:02Z" level=info msg="Stop pulling image prom/prometheus:latest: Status: Image is up to date for prom/prometheus:latest"
Dec 08 15:21:03 minikube dockerd[986]: time="2024-12-08T15:21:03.546949145Z" level=info msg="ignoring event" container=112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 15:21:03 minikube dockerd[986]: time="2024-12-08T15:21:03.769247302Z" level=info msg="ignoring event" container=f330a8d054b9db04eb5cb8f85619b21e1cae56f229145bc687691f1e2da62cfc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d6d3947abbaa1       prom/prometheus@sha256:565ee86501224ebbb98fc10b332fa54440b100469924003359edf49cbce374bd              16 seconds ago      Running             prometheus                0                   7050bcb869388       prometheus-778579754b-sjwh7
4bf6dbfbec45a       yassinezenned/fraud@sha256:beec2f6ffd2cf2671da70530969241c051d03c82d881a8ced12b94030d4fe359          20 minutes ago      Running             fraud                     5                   162affe57ae84       fraud-659ff85d7-dfxhz
208a50eaa27b0       rabbitmq@sha256:5e859a09297ff0532312d5a95d4cf87b524991b265df0b7e3111be0db1391360                     20 minutes ago      Running             rabbitmq-k8s              4                   9d83adac6f86b       rabbitmq-0
96ae627bd6852       yassinezenned/customer@sha256:84f9e6ed99183998b12167cd9721b9204379008d7a1ddc72685883e8d9cf5edd       20 minutes ago      Running             customer                  4                   0e83a0b853e32       customer-7d78dfc6b-gzg9h
6a718458c84d6       yassinezenned/notification@sha256:a2ce30ac3b965c933785f74fa6e1a53f9e36296cb088f0d60090e26a35872b67   20 minutes ago      Running             notification              4                   a5a4ee8e1a2f4       notification-6675954bc7-fm24n
a7e1ac335ec83       openzipkin/zipkin@sha256:458b8d0b51e4cc3be0ee51c816fcb3d97a316e4a37462efbe2cb5c9d2a262f5b            20 minutes ago      Running             zipkin                    4                   10caad3a2fb2e       zipkin-0
dba2ab82ec364       6e38f40d628db                                                                                        20 minutes ago      Running             storage-provisioner       16                  1110558e4e217       storage-provisioner
21962c7215719       yassinezenned/fraud@sha256:beec2f6ffd2cf2671da70530969241c051d03c82d881a8ced12b94030d4fe359          21 minutes ago      Exited              fraud                     4                   162affe57ae84       fraud-659ff85d7-dfxhz
43797904ac259       e6c5e6a76255b                                                                                        21 minutes ago      Running             postgres                  4                   f9dbfc6537ae8       postgres-0
82f55265a03ea       ead0a4a53df89                                                                                        21 minutes ago      Running             coredns                   8                   b4be2bc3cc342       coredns-5dd5756b68-ggk2k
f48d1ee9abdfc       6e38f40d628db                                                                                        21 minutes ago      Exited              storage-provisioner       15                  1110558e4e217       storage-provisioner
d47b6098246ea       bfc896cf80fba                                                                                        21 minutes ago      Running             kube-proxy                8                   5a812d59f2a30       kube-proxy-h64xt
d16a52046f231       6d1b4fd1b182d                                                                                        21 minutes ago      Running             kube-scheduler            8                   df95588049710       kube-scheduler-minikube
29222c5625133       5374347291230                                                                                        21 minutes ago      Running             kube-apiserver            8                   6c6aee9c60d02       kube-apiserver-minikube
c11bbc9a50136       73deb9a3f7025                                                                                        21 minutes ago      Running             etcd                      8                   b5626c61fd4d0       etcd-minikube
2dc544f7f71f6       10baa1ca17068                                                                                        21 minutes ago      Running             kube-controller-manager   8                   8c81abb5191ab       kube-controller-manager-minikube
9b1fd8286a683       yassinezenned/customer@sha256:84f9e6ed99183998b12167cd9721b9204379008d7a1ddc72685883e8d9cf5edd       6 days ago          Exited              customer                  3                   9dc76da523133       customer-7d78dfc6b-gzg9h
996eda0ae82fe       rabbitmq@sha256:5e859a09297ff0532312d5a95d4cf87b524991b265df0b7e3111be0db1391360                     6 days ago          Exited              rabbitmq-k8s              3                   9b1188ed1c8c2       rabbitmq-0
82c1707ad9701       openzipkin/zipkin@sha256:32599e9a997272a15a57f1775709ad86f973de98b407b9a1cbcfcb0d9b60d23b            6 days ago          Exited              zipkin                    3                   69630afd72158       zipkin-0
3fae104967bc7       yassinezenned/notification@sha256:a2ce30ac3b965c933785f74fa6e1a53f9e36296cb088f0d60090e26a35872b67   6 days ago          Exited              notification              3                   6992af9f4d988       notification-6675954bc7-fm24n
8e89ab0bbaf1e       ead0a4a53df89                                                                                        6 days ago          Exited              coredns                   7                   bcc80ebc48fcf       coredns-5dd5756b68-ggk2k
182bf9f0a5a93       e6c5e6a76255b                                                                                        6 days ago          Exited              postgres                  3                   48ed6601d0ea6       postgres-0
9906483a64a82       bfc896cf80fba                                                                                        6 days ago          Exited              kube-proxy                7                   70ba04c1d36fa       kube-proxy-h64xt
7b6138c1f0741       5374347291230                                                                                        6 days ago          Exited              kube-apiserver            7                   8d6de95066d9c       kube-apiserver-minikube
11d9b5a8dcb8b       73deb9a3f7025                                                                                        6 days ago          Exited              etcd                      7                   196fb8e8d748a       etcd-minikube
dac6984c06d4a       6d1b4fd1b182d                                                                                        6 days ago          Exited              kube-scheduler            7                   3b89802649570       kube-scheduler-minikube
6280cf4e85eea       10baa1ca17068                                                                                        6 days ago          Exited              kube-controller-manager   7                   34a26bb92c52c       kube-controller-manager-minikube

* 
* ==> coredns [82f55265a03e] <==
* [INFO] 10.244.0.78:47482 - 34172 "AAAA IN notification. udp 41 false 1232" - - 0 6.002265825s
[ERROR] plugin/errors: 2 notification. AAAA: read udp 10.244.0.71:49035->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:56229 - 41725 "A IN fraud.monitoring.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.00012586s
[INFO] 10.244.0.78:35500 - 63765 "AAAA IN fraud.monitoring.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.000216903s
[INFO] 10.244.0.78:50462 - 64722 "A IN fraud.svc.cluster.local. udp 52 false 1232" NXDOMAIN qr,aa,rd 134 0.000100081s
[INFO] 10.244.0.78:36256 - 28939 "AAAA IN fraud.svc.cluster.local. udp 52 false 1232" NXDOMAIN qr,aa,rd 134 0.000142672s
[INFO] 10.244.0.78:55293 - 31584 "A IN fraud.cluster.local. udp 48 false 1232" NXDOMAIN qr,aa,rd 130 0.000085152s
[INFO] 10.244.0.78:45890 - 44824 "AAAA IN fraud.cluster.local. udp 48 false 1232" NXDOMAIN qr,aa,rd 130 0.0001493s
[INFO] 10.244.0.78:44524 - 40559 "AAAA IN customer.monitoring.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.00012607s
[INFO] 10.244.0.78:35153 - 30116 "A IN customer.monitoring.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000196033s
[INFO] 10.244.0.78:57033 - 40858 "A IN customer.svc.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.000079011s
[INFO] 10.244.0.78:36592 - 16726 "AAAA IN customer.svc.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.000108828s
[INFO] 10.244.0.78:48739 - 3892 "A IN customer.cluster.local. udp 51 false 1232" NXDOMAIN qr,aa,rd 133 0.000081475s
[INFO] 10.244.0.78:50477 - 35630 "AAAA IN customer.cluster.local. udp 51 false 1232" NXDOMAIN qr,aa,rd 133 0.000119763s
[INFO] 10.244.0.78:51185 - 4549 "A IN notification.monitoring.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000130239s
[INFO] 10.244.0.78:57025 - 2808 "AAAA IN notification.monitoring.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000213473s
[INFO] 10.244.0.78:51909 - 11725 "AAAA IN notification.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000118566s
[INFO] 10.244.0.78:39314 - 65482 "A IN notification.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000214434s
[INFO] 10.244.0.78:45455 - 12225 "A IN notification.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.00008374s
[INFO] 10.244.0.78:37395 - 8416 "AAAA IN notification.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.000152311s
[INFO] 10.244.0.78:37890 - 25807 "AAAA IN fraud. udp 34 false 1232" - - 0 6.002625104s
[INFO] 10.244.0.78:40110 - 25150 "A IN fraud. udp 34 false 1232" - - 0 6.002674563s
[ERROR] plugin/errors: 2 fraud. A: read udp 10.244.0.71:56977->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 fraud. AAAA: read udp 10.244.0.71:49296->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:33426 - 48646 "AAAA IN customer. udp 37 false 1232" - - 0 6.001520324s
[INFO] 10.244.0.78:52317 - 14954 "A IN customer. udp 37 false 1232" - - 0 6.001568325s
[ERROR] plugin/errors: 2 customer. AAAA: read udp 10.244.0.71:58677->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 customer. A: read udp 10.244.0.71:52646->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:41695 - 48964 "AAAA IN notification. udp 41 false 1232" - - 0 6.00274305s
[INFO] 10.244.0.78:50386 - 50929 "A IN notification. udp 41 false 1232" - - 0 6.002795971s
[ERROR] plugin/errors: 2 notification. AAAA: read udp 10.244.0.71:59605->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 notification. A: read udp 10.244.0.71:36961->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:39387 - 61791 "AAAA IN fraud. udp 34 false 1232" - - 0 6.002091094s
[INFO] 10.244.0.78:59245 - 21824 "A IN fraud. udp 34 false 1232" - - 0 6.002128605s
[ERROR] plugin/errors: 2 fraud. AAAA: read udp 10.244.0.71:47807->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 fraud. A: read udp 10.244.0.71:37419->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:44143 - 39751 "A IN customer. udp 37 false 1232" - - 0 6.001834225s
[INFO] 10.244.0.78:52868 - 1190 "AAAA IN customer. udp 37 false 1232" - - 0 6.001852796s
[ERROR] plugin/errors: 2 customer. AAAA: read udp 10.244.0.71:43043->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 customer. A: read udp 10.244.0.71:49440->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.78:38608 - 26061 "A IN notification. udp 41 false 1232" - - 0 6.00278838s
[INFO] 10.244.0.78:34091 - 3264 "AAAA IN notification. udp 41 false 1232" - - 0 6.002806123s
[ERROR] plugin/errors: 2 notification. AAAA: read udp 10.244.0.71:54191->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 notification. A: read udp 10.244.0.71:50845->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.79:46836 - 63792 "A IN notification.monitoring.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000210061s
[INFO] 10.244.0.79:46061 - 37394 "AAAA IN notification.monitoring.svc.cluster.local. udp 70 false 1232" NXDOMAIN qr,aa,rd 152 0.000255668s
[INFO] 10.244.0.79:56593 - 45808 "AAAA IN notification.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000128916s
[INFO] 10.244.0.79:44648 - 18028 "A IN notification.svc.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.00019617s
[INFO] 10.244.0.79:37105 - 13470 "AAAA IN notification.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.000090673s
[INFO] 10.244.0.79:34062 - 28391 "A IN notification.cluster.local. udp 55 false 1232" NXDOMAIN qr,aa,rd 137 0.000107906s
[INFO] 10.244.0.79:46240 - 34777 "AAAA IN notification. udp 41 false 1232" - - 0 6.002009678s
[INFO] 10.244.0.79:54672 - 30811 "A IN notification. udp 41 false 1232" - - 0 6.002051538s
[ERROR] plugin/errors: 2 notification. AAAA: read udp 10.244.0.71:34297->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 notification. A: read udp 10.244.0.71:36450->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.79:55597 - 16040 "AAAA IN fraud.monitoring.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.000171698s
[INFO] 10.244.0.79:53187 - 33597 "A IN fraud.monitoring.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.000255747s
[INFO] 10.244.0.79:49869 - 32946 "AAAA IN fraud.svc.cluster.local. udp 52 false 1232" NXDOMAIN qr,aa,rd 134 0.00008368s
[INFO] 10.244.0.79:57658 - 36271 "A IN fraud.svc.cluster.local. udp 52 false 1232" NXDOMAIN qr,aa,rd 134 0.000135378s
[INFO] 10.244.0.79:38191 - 33430 "A IN fraud.cluster.local. udp 48 false 1232" NXDOMAIN qr,aa,rd 130 0.000096464s
[INFO] 10.244.0.79:50587 - 22027 "AAAA IN fraud.cluster.local. udp 48 false 1232" NXDOMAIN qr,aa,rd 130 0.000116126s

* 
* ==> coredns [8e89ab0bbaf1] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:60121 - 63569 "HINFO IN 8309681346594551506.3570427618665849343. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.180513956s
[INFO] 10.244.0.66:55164 - 3426 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000627282s
[INFO] 10.244.0.65:51033 - 32865 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000587777s
[INFO] 10.244.0.68:60249 - 2958 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.099822688s
[INFO] 10.244.0.65:57615 - 28949 "A IN postgres.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000245324s
[INFO] 10.244.0.66:40254 - 21136 "A IN postgres.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.00019456s
[INFO] 10.244.0.68:58614 - 30919 "A IN postgres.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000187628s
[INFO] 10.244.0.66:51116 - 2457 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000268403s
[INFO] 10.244.0.68:40464 - 32294 "A IN fraud.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000276367s
[INFO] 10.244.0.68:51212 - 48804 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000219066s
[INFO] 10.244.0.68:54436 - 33975 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000452306s
[INFO] 10.244.0.65:51993 - 35804 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000198744s
[INFO] 10.244.0.69:40908 - 14704 "A IN kubernetes.default.svc.cluster.local.default.svc.cluster.local. udp 80 false 512" NXDOMAIN qr,aa,rd 173 0.000368097s
[INFO] 10.244.0.69:55626 - 8803 "A IN kubernetes.default.svc.cluster.local.svc.cluster.local. udp 72 false 512" NXDOMAIN qr,aa,rd 165 0.000191998s
[INFO] 10.244.0.69:47736 - 46094 "A IN kubernetes.default.svc.cluster.local.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000168935s
[INFO] 10.244.0.69:43104 - 21652 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000175368s
[INFO] 10.244.0.66:35167 - 33477 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000307646s
[INFO] 10.244.0.68:49527 - 796 "A IN rabbitmq.default.svc.cluster.local. udp 52 false 512" NOERROR qr,aa,rd 102 0.000287281s
[INFO] 10.244.0.68:49456 - 49979 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000275528s
[INFO] 10.244.0.65:58342 - 32647 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000303771s
[INFO] 10.244.0.66:50041 - 6628 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.00022367s
[INFO] 10.244.0.68:44250 - 21689 "A IN fraud.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000435589s
[INFO] 10.244.0.66:48921 - 41574 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000351634s
[INFO] 10.244.0.68:42348 - 26804 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000213561s
[INFO] 10.244.0.65:34006 - 7766 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000183501s
[INFO] 10.244.0.68:55005 - 11917 "A IN fraud.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000154424s
[INFO] 10.244.0.68:39372 - 52318 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000176758s
[INFO] 10.244.0.65:58097 - 6902 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000113697s
[INFO] 10.244.0.66:39373 - 27184 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000113897s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] 10.244.0.68:57361 - 39532 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000244635s
[INFO] 10.244.0.65:54732 - 64996 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000096093s
[INFO] 10.244.0.66:34240 - 54778 "A IN zipkin.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000101313s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_16T16_06_40_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 16 Feb 2024 15:06:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 08 Dec 2024 15:21:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 08 Dec 2024 15:19:58 +0000   Fri, 16 Feb 2024 15:06:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 08 Dec 2024 15:19:58 +0000   Fri, 16 Feb 2024 15:06:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 08 Dec 2024 15:19:58 +0000   Fri, 16 Feb 2024 15:06:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 08 Dec 2024 15:19:58 +0000   Fri, 16 Feb 2024 15:06:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14240944Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14240944Ki
  pods:               110
System Info:
  Machine ID:                 34cec65d715e4be29172627917860377
  System UUID:                34cec65d715e4be29172627917860377
  Boot ID:                    37eb9243-bbe1-4dd7-8066-8e107f40ef57
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     customer-7d78dfc6b-gzg9h            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         289d
  default                     fraud-659ff85d7-dfxhz               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         289d
  default                     notification-6675954bc7-fm24n       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         289d
  default                     postgres-0                          100m (0%!)(MISSING)     500m (4%!)(MISSING)   256Mi (1%!)(MISSING)       512Mi (3%!)(MISSING)     289d
  default                     rabbitmq-0                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         289d
  default                     zipkin-0                            100m (0%!)(MISSING)     200m (1%!)(MISSING)   256Mi (1%!)(MISSING)       256Mi (1%!)(MISSING)     289d
  kube-system                 coredns-5dd5756b68-ggk2k            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     296d
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         296d
  kube-system                 kube-apiserver-minikube             250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         296d
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         296d
  kube-system                 kube-proxy-h64xt                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         296d
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         296d
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         296d
  monitoring                  prometheus-778579754b-sjwh7         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (7%!)(MISSING)   700m (5%!)(MISSING)
  memory             682Mi (4%!)(MISSING)  938Mi (6%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 6d5h                 kube-proxy       
  Normal  Starting                 21m                  kube-proxy       
  Normal  NodeAllocatableEnforced  6d5h                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  6d5h (x8 over 6d5h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    6d5h (x8 over 6d5h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     6d5h (x7 over 6d5h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 6d5h                 kubelet          Starting kubelet.
  Normal  RegisteredNode           6d5h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 21m                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  21m                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  21m (x8 over 21m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    21m (x8 over 21m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     21m (x7 over 21m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           21m                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.000434] FS-Cache: O-cookie d=0000000088bf6711{9P.session} n=0000000066781d00
[  +0.000416] FS-Cache: O-key=[10] '34323934393337343331'
[  +0.000272] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000329] FS-Cache: N-cookie d=0000000088bf6711{9P.session} n=00000000f9167ba0
[  +0.000554] FS-Cache: N-key=[10] '34323934393337343331'
[  +0.503804] FS-Cache: Duplicate cookie detected
[  +0.000344] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.000254] FS-Cache: O-cookie d=0000000088bf6711{9P.session} n=00000000de0feadd
[  +0.000297] FS-Cache: O-key=[10] '34323934393337343832'
[  +0.000204] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.000290] FS-Cache: N-cookie d=0000000088bf6711{9P.session} n=0000000085ec18ca
[  +0.000301] FS-Cache: N-key=[10] '34323934393337343832'
[  +0.018060] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.004715] FS-Cache: Duplicate cookie detected
[  +0.000353] FS-Cache: O-cookie c=0000000d [p=00000002 fl=222 nc=0 na=1]
[  +0.000342] FS-Cache: O-cookie d=0000000088bf6711{9P.session} n=000000002ed2c1a7
[  +0.000469] FS-Cache: O-key=[10] '34323934393337343834'
[  +0.000281] FS-Cache: N-cookie c=0000000e [p=00000002 fl=2 nc=0 na=1]
[  +0.000373] FS-Cache: N-cookie d=0000000088bf6711{9P.session} n=000000005a2cb645
[  +0.000377] FS-Cache: N-key=[10] '34323934393337343834'
[  +0.009914] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.302747] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.010457] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000651] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000689] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003716] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000608] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000513] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000754] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.256062] FS-Cache: Duplicate cookie detected
[  +0.000473] FS-Cache: O-cookie c=00000013 [p=00000002 fl=222 nc=0 na=1]
[  +0.000493] FS-Cache: O-cookie d=0000000088bf6711{9P.session} n=00000000467e8cb5
[  +0.000562] FS-Cache: O-key=[10] '34323934393337353433'
[  +0.000322] FS-Cache: N-cookie c=00000014 [p=00000002 fl=2 nc=0 na=1]
[  +0.000464] FS-Cache: N-cookie d=0000000088bf6711{9P.session} n=00000000ebe19984
[  +0.000588] FS-Cache: N-key=[10] '34323934393337353433'
[  +0.007830] WSL (2) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.000909] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.000963] WSL (1) ERROR: ConfigMountFsTab:2583: Processing fstab with mount -a failed.
[  +0.001818] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.003114] WSL (3) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.000846] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.002137] WSL (4) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.000908] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.081555] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.019595] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002620] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001309] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000732] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002010] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002691] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000569] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000604] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000627] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2

* 
* ==> etcd [11d9b5a8dcb8] <==
* {"level":"warn","ts":"2024-12-02T09:33:03.341518Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T09:33:02.741555Z","time spent":"599.954325ms","remote":"127.0.0.1:46086","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-12-02T09:33:04.130553Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.442982ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033634266764875 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" mod_revision:72498 > success:<request_put:<key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" value_size:653 lease:8128033634266764873 >> failure:<request_range:<key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-02T09:33:04.130688Z","caller":"traceutil/trace.go:171","msg":"trace[2131943973] transaction","detail":"{read_only:false; response_revision:72545; number_of_response:1; }","duration":"300.136664ms","start":"2024-12-02T09:33:03.830529Z","end":"2024-12-02T09:33:04.130665Z","steps":["trace[2131943973] 'process raft request'  (duration: 99.493269ms)","trace[2131943973] 'compare'  (duration: 200.221549ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-02T09:33:04.130814Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T09:33:03.830505Z","time spent":"300.260481ms","remote":"127.0.0.1:46138","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":723,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" mod_revision:72498 > success:<request_put:<key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" value_size:653 lease:8128033634266764873 >> failure:<request_range:<key:\"/registry/events/default/rabbitmq-0.180d524d3ff38008\" > >"}
{"level":"warn","ts":"2024-12-02T09:33:04.740287Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"398.895495ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-12-02T09:33:04.740383Z","caller":"traceutil/trace.go:171","msg":"trace[1809993099] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:72545; }","duration":"399.005214ms","start":"2024-12-02T09:33:04.341356Z","end":"2024-12-02T09:33:04.740362Z","steps":["trace[1809993099] 'range keys from in-memory index tree'  (duration: 398.702185ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:04.740432Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T09:33:04.341337Z","time spent":"399.082783ms","remote":"127.0.0.1:46244","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-12-02T09:33:04.740759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"303.433381ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:8 size:8523"}
{"level":"info","ts":"2024-12-02T09:33:04.7408Z","caller":"traceutil/trace.go:171","msg":"trace[504621379] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:8; response_revision:72545; }","duration":"303.477776ms","start":"2024-12-02T09:33:04.437312Z","end":"2024-12-02T09:33:04.74079Z","steps":["trace[504621379] 'range keys from in-memory index tree'  (duration: 303.252095ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:04.740838Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T09:33:04.437292Z","time spent":"303.535556ms","remote":"127.0.0.1:46262","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":8,"response size":8547,"request content":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" "}
{"level":"warn","ts":"2024-12-02T09:33:05.230596Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"280.833405ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033634266764882 > lease_revoke:<id:70cc9386b4e5d5ff>","response":"size:30"}
{"level":"info","ts":"2024-12-02T09:33:05.935603Z","caller":"traceutil/trace.go:171","msg":"trace[1815026958] transaction","detail":"{read_only:false; response_revision:72547; number_of_response:1; }","duration":"105.142041ms","start":"2024-12-02T09:33:05.830428Z","end":"2024-12-02T09:33:05.93557Z","steps":["trace[1815026958] 'process raft request'  (duration: 104.942313ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:06.331072Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"394.059467ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:8 size:8523"}
{"level":"info","ts":"2024-12-02T09:33:06.331169Z","caller":"traceutil/trace.go:171","msg":"trace[1266201961] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:8; response_revision:72547; }","duration":"394.172658ms","start":"2024-12-02T09:33:05.936978Z","end":"2024-12-02T09:33:06.331151Z","steps":["trace[1266201961] 'range keys from in-memory index tree'  (duration: 393.877666ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:06.331222Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T09:33:05.93696Z","time spent":"394.249164ms","remote":"127.0.0.1:46262","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":8,"response size":8547,"request content":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" "}
{"level":"info","ts":"2024-12-02T09:33:07.23186Z","caller":"traceutil/trace.go:171","msg":"trace[2078517567] transaction","detail":"{read_only:false; response_revision:72548; number_of_response:1; }","duration":"101.083004ms","start":"2024-12-02T09:33:07.130732Z","end":"2024-12-02T09:33:07.231815Z","steps":["trace[2078517567] 'process raft request'  (duration: 100.728681ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:07.54133Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.557027ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-02T09:33:07.541412Z","caller":"traceutil/trace.go:171","msg":"trace[1594024468] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:72548; }","duration":"110.652111ms","start":"2024-12-02T09:33:07.430741Z","end":"2024-12-02T09:33:07.541393Z","steps":["trace[1594024468] 'count revisions from in-memory index tree'  (duration: 110.434156ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:09.730299Z","caller":"traceutil/trace.go:171","msg":"trace[1811118432] transaction","detail":"{read_only:false; response_revision:72549; number_of_response:1; }","duration":"187.238545ms","start":"2024-12-02T09:33:09.543035Z","end":"2024-12-02T09:33:09.730273Z","steps":["trace[1811118432] 'process raft request'  (duration: 187.069319ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:10.142753Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.640839ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-02T09:33:10.14285Z","caller":"traceutil/trace.go:171","msg":"trace[1107326873] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:72550; }","duration":"211.769282ms","start":"2024-12-02T09:33:09.931062Z","end":"2024-12-02T09:33:10.142831Z","steps":["trace[1107326873] 'count revisions from in-memory index tree'  (duration: 211.485362ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:10.143248Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.739802ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:421"}
{"level":"info","ts":"2024-12-02T09:33:10.143302Z","caller":"traceutil/trace.go:171","msg":"trace[1918787714] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:72550; }","duration":"211.799002ms","start":"2024-12-02T09:33:09.93149Z","end":"2024-12-02T09:33:10.143289Z","steps":["trace[1918787714] 'range keys from in-memory index tree'  (duration: 211.105056ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:12.030855Z","caller":"traceutil/trace.go:171","msg":"trace[1745200461] transaction","detail":"{read_only:false; response_revision:72551; number_of_response:1; }","duration":"180.302859ms","start":"2024-12-02T09:33:11.850516Z","end":"2024-12-02T09:33:12.030819Z","steps":["trace[1745200461] 'process raft request'  (duration: 179.604504ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:13.04705Z","caller":"traceutil/trace.go:171","msg":"trace[168427228] transaction","detail":"{read_only:false; response_revision:72552; number_of_response:1; }","duration":"104.175127ms","start":"2024-12-02T09:33:12.942849Z","end":"2024-12-02T09:33:13.047024Z","steps":["trace[168427228] 'process raft request'  (duration: 104.016884ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:14.431005Z","caller":"traceutil/trace.go:171","msg":"trace[1617951270] transaction","detail":"{read_only:false; response_revision:72554; number_of_response:1; }","duration":"187.764074ms","start":"2024-12-02T09:33:14.243205Z","end":"2024-12-02T09:33:14.430969Z","steps":["trace[1617951270] 'process raft request'  (duration: 187.529221ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:15.23921Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"203.044077ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:8 size:8523"}
{"level":"info","ts":"2024-12-02T09:33:15.23944Z","caller":"traceutil/trace.go:171","msg":"trace[1633422759] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:8; response_revision:72554; }","duration":"203.315896ms","start":"2024-12-02T09:33:15.036088Z","end":"2024-12-02T09:33:15.239404Z","steps":["trace[1633422759] 'range keys from in-memory index tree'  (duration: 202.765148ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:19.23176Z","caller":"traceutil/trace.go:171","msg":"trace[1610591693] transaction","detail":"{read_only:false; response_revision:72556; number_of_response:1; }","duration":"200.087505ms","start":"2024-12-02T09:33:19.031642Z","end":"2024-12-02T09:33:19.231729Z","steps":["trace[1610591693] 'process raft request'  (duration: 199.684897ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:19.846224Z","caller":"traceutil/trace.go:171","msg":"trace[2133710840] transaction","detail":"{read_only:false; response_revision:72557; number_of_response:1; }","duration":"114.5501ms","start":"2024-12-02T09:33:19.731639Z","end":"2024-12-02T09:33:19.846189Z","steps":["trace[2133710840] 'process raft request'  (duration: 99.924566ms)","trace[2133710840] 'compare'  (duration: 14.436957ms)"],"step_count":2}
{"level":"info","ts":"2024-12-02T09:33:20.542577Z","caller":"traceutil/trace.go:171","msg":"trace[686151648] transaction","detail":"{read_only:false; response_revision:72558; number_of_response:1; }","duration":"101.343656ms","start":"2024-12-02T09:33:20.441205Z","end":"2024-12-02T09:33:20.542549Z","steps":["trace[686151648] 'process raft request'  (duration: 100.771131ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:21.531285Z","caller":"traceutil/trace.go:171","msg":"trace[1428940983] transaction","detail":"{read_only:false; response_revision:72559; number_of_response:1; }","duration":"187.680526ms","start":"2024-12-02T09:33:21.343579Z","end":"2024-12-02T09:33:21.531259Z","steps":["trace[1428940983] 'process raft request'  (duration: 187.514962ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:21.731412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.204137ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-02T09:33:21.73151Z","caller":"traceutil/trace.go:171","msg":"trace[1729118009] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:72559; }","duration":"195.324836ms","start":"2024-12-02T09:33:21.536165Z","end":"2024-12-02T09:33:21.73149Z","steps":["trace[1729118009] 'count revisions from in-memory index tree'  (duration: 195.078259ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:23.732303Z","caller":"traceutil/trace.go:171","msg":"trace[354273441] transaction","detail":"{read_only:false; response_revision:72560; number_of_response:1; }","duration":"197.733678ms","start":"2024-12-02T09:33:23.534545Z","end":"2024-12-02T09:33:23.732279Z","steps":["trace[354273441] 'process raft request'  (duration: 197.567363ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:24.442736Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"298.020582ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:8 size:8523"}
{"level":"info","ts":"2024-12-02T09:33:24.442876Z","caller":"traceutil/trace.go:171","msg":"trace[1991349106] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:8; response_revision:72561; }","duration":"298.172637ms","start":"2024-12-02T09:33:24.144676Z","end":"2024-12-02T09:33:24.442849Z","steps":["trace[1991349106] 'range keys from in-memory index tree'  (duration: 297.80844ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T09:33:24.531402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.017331ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033634266764977 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:72553 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-02T09:33:24.531512Z","caller":"traceutil/trace.go:171","msg":"trace[1089961677] transaction","detail":"{read_only:false; response_revision:72562; number_of_response:1; }","duration":"189.421618ms","start":"2024-12-02T09:33:24.34207Z","end":"2024-12-02T09:33:24.531491Z","steps":["trace[1089961677] 'process raft request'  (duration: 89.221276ms)","trace[1089961677] 'compare'  (duration: 11.255991ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-02T09:33:24.737929Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.288385ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-02T09:33:24.738025Z","caller":"traceutil/trace.go:171","msg":"trace[283157217] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:72562; }","duration":"105.403511ms","start":"2024-12-02T09:33:24.6326Z","end":"2024-12-02T09:33:24.738004Z","steps":["trace[283157217] 'range keys from in-memory index tree'  (duration: 105.159309ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:33:28.347149Z","caller":"traceutil/trace.go:171","msg":"trace[664075049] transaction","detail":"{read_only:false; response_revision:72564; number_of_response:1; }","duration":"193.809523ms","start":"2024-12-02T09:33:28.153299Z","end":"2024-12-02T09:33:28.347108Z","steps":["trace[664075049] 'process raft request'  (duration: 178.72633ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:35:21.847407Z","caller":"traceutil/trace.go:171","msg":"trace[102012320] transaction","detail":"{read_only:false; response_revision:72659; number_of_response:1; }","duration":"103.266333ms","start":"2024-12-02T09:35:21.744001Z","end":"2024-12-02T09:35:21.847268Z","steps":["trace[102012320] 'process raft request'  (duration: 100.791573ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T09:40:23.206059Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":72659}
{"level":"info","ts":"2024-12-02T09:40:23.220904Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":72659,"took":"14.660189ms","hash":3947164943}
{"level":"info","ts":"2024-12-02T09:40:23.220955Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3947164943,"revision":72659,"compact-revision":71810}
{"level":"info","ts":"2024-12-02T09:45:23.214414Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":72899}
{"level":"info","ts":"2024-12-02T09:45:23.21518Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":72899,"took":"549.001s","hash":412965492}
{"level":"info","ts":"2024-12-02T09:45:23.215215Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":412965492,"revision":72899,"compact-revision":72659}
{"level":"info","ts":"2024-12-02T09:48:19.13427Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-12-02T09:48:19.134413Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-12-02T09:48:19.13457Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-12-02T09:48:19.13469Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
WARNING: 2024/12/02 09:48:19 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-12-02T09:48:19.145574Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-12-02T09:48:19.145658Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-12-02T09:48:19.145752Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-02T09:48:19.243387Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-02T09:48:19.243553Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-02T09:48:19.243571Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [c11bbc9a5013] <==
* {"level":"info","ts":"2024-12-08T14:59:59.406934Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 10"}
{"level":"info","ts":"2024-12-08T14:59:59.410582Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-08T14:59:59.41061Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-08T14:59:59.410581Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-08T14:59:59.410949Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-08T14:59:59.411004Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-08T14:59:59.411576Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-12-08T14:59:59.411979Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-08T15:00:14.474862Z","caller":"traceutil/trace.go:171","msg":"trace[1772246653] linearizableReadLoop","detail":"{readStateIndex:91476; appliedIndex:91471; }","duration":"100.137198ms","start":"2024-12-08T15:00:14.374661Z","end":"2024-12-08T15:00:14.474798Z","steps":["trace[1772246653] 'read index received'  (duration: 7.652442ms)","trace[1772246653] 'applied index is now lower than readState.Index'  (duration: 92.482341ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-08T15:00:14.475141Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.441552ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/customer\" ","response":"range_response_count:1 size:251"}
{"level":"info","ts":"2024-12-08T15:00:14.475212Z","caller":"traceutil/trace.go:171","msg":"trace[433717158] range","detail":"{range_begin:/registry/services/endpoints/default/customer; range_end:; response_count:1; response_revision:73407; }","duration":"100.545239ms","start":"2024-12-08T15:00:14.374642Z","end":"2024-12-08T15:00:14.475187Z","steps":["trace[433717158] 'agreement among raft nodes before linearized reading'  (duration: 100.197888ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:11.477177Z","caller":"traceutil/trace.go:171","msg":"trace[1968870237] transaction","detail":"{read_only:false; response_revision:73498; number_of_response:1; }","duration":"101.377472ms","start":"2024-12-08T15:01:11.375712Z","end":"2024-12-08T15:01:11.47709Z","steps":["trace[1968870237] 'process raft request'  (duration: 99.479291ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:20.470901Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.854922ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:20.470991Z","caller":"traceutil/trace.go:171","msg":"trace[708464927] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:73508; }","duration":"190.961776ms","start":"2024-12-08T15:01:20.28001Z","end":"2024-12-08T15:01:20.470972Z","steps":["trace[708464927] 'range keys from in-memory index tree'  (duration: 190.752236ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:23.274988Z","caller":"traceutil/trace.go:171","msg":"trace[780764689] transaction","detail":"{read_only:false; response_revision:73512; number_of_response:1; }","duration":"103.387762ms","start":"2024-12-08T15:01:23.171574Z","end":"2024-12-08T15:01:23.274961Z","steps":["trace[780764689] 'process raft request'  (duration: 99.109409ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:26.772949Z","caller":"traceutil/trace.go:171","msg":"trace[378511398] transaction","detail":"{read_only:false; response_revision:73514; number_of_response:1; }","duration":"101.643374ms","start":"2024-12-08T15:01:26.671282Z","end":"2024-12-08T15:01:26.772926Z","steps":["trace[378511398] 'process raft request'  (duration: 101.515541ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:33.770715Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"299.000152ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-08T15:01:33.770795Z","caller":"traceutil/trace.go:171","msg":"trace[599344412] range","detail":"{range_begin:/registry/controllerrevisions/; range_end:/registry/controllerrevisions0; response_count:0; response_revision:73520; }","duration":"299.100393ms","start":"2024-12-08T15:01:33.471675Z","end":"2024-12-08T15:01:33.770775Z","steps":["trace[599344412] 'count revisions from in-memory index tree'  (duration: 298.907125ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:33.771047Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"289.92138ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:33.771089Z","caller":"traceutil/trace.go:171","msg":"trace[1052059630] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:73520; }","duration":"289.970543ms","start":"2024-12-08T15:01:33.481109Z","end":"2024-12-08T15:01:33.771079Z","steps":["trace[1052059630] 'range keys from in-memory index tree'  (duration: 289.811561ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:39.67256Z","caller":"traceutil/trace.go:171","msg":"trace[200238528] transaction","detail":"{read_only:false; response_revision:73524; number_of_response:1; }","duration":"196.027958ms","start":"2024-12-08T15:01:39.476512Z","end":"2024-12-08T15:01:39.67254Z","steps":["trace[200238528] 'process raft request'  (duration: 195.578499ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:39.97087Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"296.879002ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:39.970963Z","caller":"traceutil/trace.go:171","msg":"trace[865873786] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:73524; }","duration":"296.996716ms","start":"2024-12-08T15:01:39.673948Z","end":"2024-12-08T15:01:39.970945Z","steps":["trace[865873786] 'range keys from in-memory index tree'  (duration: 296.79473ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:43.482849Z","caller":"traceutil/trace.go:171","msg":"trace[1024489587] transaction","detail":"{read_only:false; response_revision:73528; number_of_response:1; }","duration":"102.423545ms","start":"2024-12-08T15:01:43.380406Z","end":"2024-12-08T15:01:43.482829Z","steps":["trace[1024489587] 'process raft request'  (duration: 101.812071ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:48.571918Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.450551ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033772039912859 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:73530 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-08T15:01:48.572034Z","caller":"traceutil/trace.go:171","msg":"trace[721354994] transaction","detail":"{read_only:false; response_revision:73531; number_of_response:1; }","duration":"188.641568ms","start":"2024-12-08T15:01:48.383374Z","end":"2024-12-08T15:01:48.572016Z","steps":["trace[721354994] 'process raft request'  (duration: 87.368161ms)","trace[721354994] 'compare'  (duration: 100.048086ms)"],"step_count":2}
{"level":"info","ts":"2024-12-08T15:01:52.97115Z","caller":"traceutil/trace.go:171","msg":"trace[1293683031] transaction","detail":"{read_only:false; response_revision:73533; number_of_response:1; }","duration":"100.15746ms","start":"2024-12-08T15:01:52.870964Z","end":"2024-12-08T15:01:52.971121Z","steps":["trace[1293683031] 'process raft request'  (duration: 99.980413ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:53.374098Z","caller":"traceutil/trace.go:171","msg":"trace[1896975720] transaction","detail":"{read_only:false; response_revision:73535; number_of_response:1; }","duration":"101.034919ms","start":"2024-12-08T15:01:53.27304Z","end":"2024-12-08T15:01:53.374075Z","steps":["trace[1896975720] 'process raft request'  (duration: 97.644037ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:53.670742Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.822164ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:53.670836Z","caller":"traceutil/trace.go:171","msg":"trace[896705079] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:73535; }","duration":"194.932254ms","start":"2024-12-08T15:01:53.475887Z","end":"2024-12-08T15:01:53.670819Z","steps":["trace[896705079] 'range keys from in-memory index tree'  (duration: 194.720881ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:55.580914Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.8476ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:55.5811Z","caller":"traceutil/trace.go:171","msg":"trace[949813299] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:73537; }","duration":"102.050005ms","start":"2024-12-08T15:01:55.479031Z","end":"2024-12-08T15:01:55.581081Z","steps":["trace[949813299] 'range keys from in-memory index tree'  (duration: 101.751246ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:01:58.780501Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.201197ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:01:58.780588Z","caller":"traceutil/trace.go:171","msg":"trace[236586805] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:73538; }","duration":"108.306417ms","start":"2024-12-08T15:01:58.672263Z","end":"2024-12-08T15:01:58.78057Z","steps":["trace[236586805] 'range keys from in-memory index tree'  (duration: 108.105354ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:01:59.878509Z","caller":"traceutil/trace.go:171","msg":"trace[231306406] transaction","detail":"{read_only:false; response_revision:73539; number_of_response:1; }","duration":"105.706862ms","start":"2024-12-08T15:01:59.772781Z","end":"2024-12-08T15:01:59.878487Z","steps":["trace[231306406] 'process raft request'  (duration: 105.554873ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:03.480999Z","caller":"traceutil/trace.go:171","msg":"trace[576057678] transaction","detail":"{read_only:false; response_revision:73542; number_of_response:1; }","duration":"107.639351ms","start":"2024-12-08T15:02:03.373341Z","end":"2024-12-08T15:02:03.48098Z","steps":["trace[576057678] 'process raft request'  (duration: 107.58122ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:03.481099Z","caller":"traceutil/trace.go:171","msg":"trace[243653193] transaction","detail":"{read_only:false; response_revision:73541; number_of_response:1; }","duration":"198.261545ms","start":"2024-12-08T15:02:03.28282Z","end":"2024-12-08T15:02:03.481081Z","steps":["trace[243653193] 'process raft request'  (duration: 99.071568ms)","trace[243653193] 'compare'  (duration: 98.921868ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-08T15:02:03.780788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.726459ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:8 size:8523"}
{"level":"info","ts":"2024-12-08T15:02:03.780877Z","caller":"traceutil/trace.go:171","msg":"trace[113039933] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:8; response_revision:73542; }","duration":"108.823914ms","start":"2024-12-08T15:02:03.672035Z","end":"2024-12-08T15:02:03.780859Z","steps":["trace[113039933] 'range keys from in-memory index tree'  (duration: 108.555104ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:04.182204Z","caller":"traceutil/trace.go:171","msg":"trace[688832795] transaction","detail":"{read_only:false; response_revision:73543; number_of_response:1; }","duration":"100.36649ms","start":"2024-12-08T15:02:04.081814Z","end":"2024-12-08T15:02:04.18218Z","steps":["trace[688832795] 'process raft request'  (duration: 99.789343ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:02:04.471007Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.018968ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033772039912949 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:73540 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-08T15:02:04.471095Z","caller":"traceutil/trace.go:171","msg":"trace[594922230] transaction","detail":"{read_only:false; response_revision:73544; number_of_response:1; }","duration":"189.395825ms","start":"2024-12-08T15:02:04.281686Z","end":"2024-12-08T15:02:04.471082Z","steps":["trace[594922230] 'compare'  (duration: 184.860747ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:08.877351Z","caller":"traceutil/trace.go:171","msg":"trace[1914739665] linearizableReadLoop","detail":"{readStateIndex:91644; appliedIndex:91642; }","duration":"190.25944ms","start":"2024-12-08T15:02:08.687069Z","end":"2024-12-08T15:02:08.877329Z","steps":["trace[1914739665] 'read index received'  (duration: 83.747801ms)","trace[1914739665] 'applied index is now lower than readState.Index'  (duration: 106.510327ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-08T15:02:08.877515Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.488484ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-08T15:02:08.877549Z","caller":"traceutil/trace.go:171","msg":"trace[63158300] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:73546; }","duration":"190.539451ms","start":"2024-12-08T15:02:08.687Z","end":"2024-12-08T15:02:08.877539Z","steps":["trace[63158300] 'agreement among raft nodes before linearized reading'  (duration: 190.460531ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-08T15:02:11.178539Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.721317ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-08T15:02:11.178612Z","caller":"traceutil/trace.go:171","msg":"trace[288097037] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:73547; }","duration":"103.810196ms","start":"2024-12-08T15:02:11.074788Z","end":"2024-12-08T15:02:11.178598Z","steps":["trace[288097037] 'count revisions from in-memory index tree'  (duration: 103.628321ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:13.186022Z","caller":"traceutil/trace.go:171","msg":"trace[1536193931] transaction","detail":"{read_only:false; response_revision:73549; number_of_response:1; }","duration":"105.416426ms","start":"2024-12-08T15:02:13.080592Z","end":"2024-12-08T15:02:13.186008Z","steps":["trace[1536193931] 'process raft request'  (duration: 104.64863ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:13.483982Z","caller":"traceutil/trace.go:171","msg":"trace[1632861896] transaction","detail":"{read_only:false; response_revision:73550; number_of_response:1; }","duration":"100.687536ms","start":"2024-12-08T15:02:13.383272Z","end":"2024-12-08T15:02:13.48396Z","steps":["trace[1632861896] 'process raft request'  (duration: 95.863179ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:14.082946Z","caller":"traceutil/trace.go:171","msg":"trace[591210403] transaction","detail":"{read_only:false; response_revision:73551; number_of_response:1; }","duration":"111.184511ms","start":"2024-12-08T15:02:13.971734Z","end":"2024-12-08T15:02:14.082919Z","steps":["trace[591210403] 'process raft request'  (duration: 111.024207ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:02:21.806982Z","caller":"traceutil/trace.go:171","msg":"trace[2124468020] transaction","detail":"{read_only:false; response_revision:73556; number_of_response:1; }","duration":"126.189652ms","start":"2024-12-08T15:02:21.680773Z","end":"2024-12-08T15:02:21.806963Z","steps":["trace[2124468020] 'process raft request'  (duration: 126.038374ms)"],"step_count":1}
{"level":"info","ts":"2024-12-08T15:09:59.44438Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":73688}
{"level":"info","ts":"2024-12-08T15:09:59.463895Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":73688,"took":"19.110985ms","hash":1517834302}
{"level":"info","ts":"2024-12-08T15:09:59.463945Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1517834302,"revision":73688,"compact-revision":72899}
{"level":"info","ts":"2024-12-08T15:14:59.449274Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":73927}
{"level":"info","ts":"2024-12-08T15:14:59.450387Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":73927,"took":"692.086s","hash":2191679655}
{"level":"info","ts":"2024-12-08T15:14:59.450426Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2191679655,"revision":73927,"compact-revision":73688}
{"level":"info","ts":"2024-12-08T15:19:59.463169Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":74168}
{"level":"info","ts":"2024-12-08T15:19:59.464035Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":74168,"took":"595.004s","hash":67599889}
{"level":"info","ts":"2024-12-08T15:19:59.464071Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":67599889,"revision":74168,"compact-revision":73927}

* 
* ==> kernel <==
*  15:21:18 up 24 min,  0 users,  load average: 0.46, 0.61, 0.55
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [29222c562513] <==
* I1208 15:00:00.135819       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1208 15:00:00.135860       1 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1208 15:00:00.629570       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1208 15:00:00.629595       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1208 15:00:00.630062       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1208 15:00:00.630857       1 secure_serving.go:213] Serving securely on [::]:8443
I1208 15:00:00.631058       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I1208 15:00:00.631099       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1208 15:00:00.631179       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1208 15:00:00.631197       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I1208 15:00:00.631198       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1208 15:00:00.631210       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1208 15:00:00.631236       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1208 15:00:00.631254       1 system_namespaces_controller.go:67] Starting system namespaces controller
I1208 15:00:00.631271       1 controller.go:78] Starting OpenAPI AggregationController
I1208 15:00:00.631297       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1208 15:00:00.631541       1 controller.go:116] Starting legacy_token_tracking_controller
I1208 15:00:00.631576       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1208 15:00:00.631216       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I1208 15:00:00.631753       1 aggregator.go:164] waiting for initial CRD sync...
I1208 15:00:00.631811       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1208 15:00:00.631946       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I1208 15:00:00.631994       1 controller.go:134] Starting OpenAPI controller
I1208 15:00:00.632017       1 controller.go:85] Starting OpenAPI V3 controller
I1208 15:00:00.632038       1 naming_controller.go:291] Starting NamingConditionController
I1208 15:00:00.632280       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1208 15:00:00.632310       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1208 15:00:00.632595       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1208 15:00:00.642901       1 establishing_controller.go:76] Starting EstablishingController
I1208 15:00:00.642958       1 crd_finalizer.go:266] Starting CRDFinalizer
I1208 15:00:00.642900       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1208 15:00:00.643169       1 available_controller.go:423] Starting AvailableConditionController
I1208 15:00:00.643258       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1208 15:00:00.643465       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1208 15:00:00.643729       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1208 15:00:00.647389       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1208 15:00:00.693204       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1208 15:00:00.731208       1 apf_controller.go:377] Running API Priority and Fairness config worker
I1208 15:00:00.731266       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I1208 15:00:00.731279       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1208 15:00:00.731319       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1208 15:00:00.731707       1 shared_informer.go:318] Caches are synced for configmaps
I1208 15:00:00.732660       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1208 15:00:00.732689       1 aggregator.go:166] initial CRD sync complete...
I1208 15:00:00.732695       1 autoregister_controller.go:141] Starting autoregister controller
I1208 15:00:00.732700       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1208 15:00:00.732706       1 cache.go:39] Caches are synced for autoregister controller
I1208 15:00:00.771570       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1208 15:00:00.772547       1 shared_informer.go:318] Caches are synced for node_authorizer
I1208 15:00:01.676721       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1208 15:00:03.409696       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1208 15:00:03.418703       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1208 15:00:03.449309       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1208 15:00:03.471636       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1208 15:00:03.478332       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1208 15:00:14.284130       1 controller.go:624] quota admission added evaluator for: endpoints
I1208 15:00:14.290173       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1208 15:17:06.428895       1 controller.go:624] quota admission added evaluator for: namespaces
I1208 15:18:37.867171       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1208 15:18:37.891777       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus" clusterIPs={"IPv4":"10.101.64.241"}

* 
* ==> kube-apiserver [7b6138c1f074] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.140965       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141068       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141098       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141136       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141209       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141210       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1202 09:48:20.141219       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [2dc544f7f71f] <==
* I1208 15:00:14.274542       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1208 15:00:14.274601       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1208 15:00:14.274668       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1208 15:00:14.274782       1 taint_manager.go:211] "Sending events to api server"
I1208 15:00:14.274210       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1208 15:00:14.274254       1 shared_informer.go:318] Caches are synced for daemon sets
I1208 15:00:14.275039       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1208 15:00:14.275923       1 shared_informer.go:318] Caches are synced for job
I1208 15:00:14.281288       1 shared_informer.go:318] Caches are synced for disruption
I1208 15:00:14.371615       1 shared_informer.go:318] Caches are synced for resource quota
I1208 15:00:14.371814       1 shared_informer.go:318] Caches are synced for cronjob
I1208 15:00:14.376794       1 shared_informer.go:318] Caches are synced for resource quota
I1208 15:00:14.378035       1 shared_informer.go:318] Caches are synced for service account
I1208 15:00:14.389677       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="116.444742ms"
I1208 15:00:14.390029       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/notification-6675954bc7" duration="115.901139ms"
I1208 15:00:14.390137       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/notification-6675954bc7" duration="72.382s"
I1208 15:00:14.390223       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="116.069949ms"
I1208 15:00:14.391164       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="148.953s"
I1208 15:00:14.391210       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="27.452s"
I1208 15:00:14.406232       1 shared_informer.go:318] Caches are synced for namespace
I1208 15:00:14.477411       1 event.go:307] "Event occurred" object="default/customer" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/customer: Operation cannot be fulfilled on endpoints \"customer\": the object has been modified; please apply your changes to the latest version and try again"
I1208 15:00:14.702943       1 shared_informer.go:318] Caches are synced for garbage collector
I1208 15:00:14.702979       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1208 15:00:14.785465       1 shared_informer.go:318] Caches are synced for garbage collector
I1208 15:00:21.893789       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="7.023355ms"
I1208 15:00:21.893891       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="48.893s"
I1208 15:00:23.854589       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="6.512744ms"
I1208 15:00:23.854695       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="41.038s"
I1208 15:00:37.784003       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="123.996s"
I1208 15:01:05.793578       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/notification-6675954bc7" duration="6.512627ms"
I1208 15:01:05.793672       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/notification-6675954bc7" duration="37.893s"
I1208 15:01:08.088502       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="15.662076ms"
I1208 15:01:08.088663       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="80.133s"
I1208 15:01:11.371048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="99.868117ms"
I1208 15:01:11.371164       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="66.697s"
I1208 15:18:37.874564       1 event.go:307] "Event occurred" object="monitoring/prometheus" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set prometheus-5986749c4b to 1"
I1208 15:18:37.886413       1 event.go:307] "Event occurred" object="monitoring/prometheus-5986749c4b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: prometheus-5986749c4b-xk948"
I1208 15:18:37.892010       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="17.68476ms"
I1208 15:18:37.902453       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="10.347015ms"
I1208 15:18:37.902560       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="60.625s"
I1208 15:18:37.915336       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="80.843s"
I1208 15:19:36.314301       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="5.319279ms"
I1208 15:19:36.314381       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="30.208s"
I1208 15:20:58.705676       1 event.go:307] "Event occurred" object="monitoring/prometheus" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set prometheus-778579754b to 1"
I1208 15:20:58.714253       1 event.go:307] "Event occurred" object="monitoring/prometheus-778579754b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: prometheus-778579754b-sjwh7"
I1208 15:20:58.719649       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="14.088979ms"
I1208 15:20:58.726253       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="6.548284ms"
I1208 15:20:58.726415       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="51.107s"
I1208 15:20:58.735107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="65.024s"
I1208 15:21:03.499402       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="8.590224ms"
I1208 15:21:03.499515       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-778579754b" duration="44.936s"
I1208 15:21:03.504438       1 event.go:307] "Event occurred" object="monitoring/prometheus" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set prometheus-5986749c4b to 0 from 1"
I1208 15:21:03.511628       1 event.go:307] "Event occurred" object="monitoring/prometheus-5986749c4b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: prometheus-5986749c4b-xk948"
I1208 15:21:03.522003       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="17.77747ms"
I1208 15:21:03.526257       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="4.178264ms"
I1208 15:21:03.526391       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="62.018s"
I1208 15:21:03.814307       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="51.278s"
I1208 15:21:04.506944       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="37.912s"
I1208 15:21:04.516726       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="94.43s"
I1208 15:21:04.519753       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/prometheus-5986749c4b" duration="45.417s"

* 
* ==> kube-controller-manager [6280cf4e85ee] <==
* I1202 09:30:42.041148       1 shared_informer.go:318] Caches are synced for namespace
I1202 09:30:42.041585       1 shared_informer.go:318] Caches are synced for service account
I1202 09:30:42.049901       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1202 09:30:42.051467       1 shared_informer.go:318] Caches are synced for node
I1202 09:30:42.051537       1 range_allocator.go:174] "Sending events to api server"
I1202 09:30:42.051604       1 range_allocator.go:178] "Starting range CIDR allocator"
I1202 09:30:42.051621       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1202 09:30:42.051635       1 shared_informer.go:318] Caches are synced for cidrallocator
I1202 09:30:42.061832       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1202 09:30:42.063276       1 shared_informer.go:318] Caches are synced for crt configmap
I1202 09:30:42.134599       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1202 09:30:42.135880       1 shared_informer.go:318] Caches are synced for job
I1202 09:30:42.136144       1 shared_informer.go:318] Caches are synced for PV protection
I1202 09:30:42.143732       1 shared_informer.go:318] Caches are synced for TTL after finished
I1202 09:30:42.143788       1 shared_informer.go:318] Caches are synced for ReplicationController
I1202 09:30:42.143813       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1202 09:30:42.144386       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1202 09:30:42.144643       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1202 09:30:42.144917       1 shared_informer.go:318] Caches are synced for cronjob
I1202 09:30:42.145053       1 shared_informer.go:318] Caches are synced for TTL
I1202 09:30:42.145297       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/notification-6675954bc7" duration="150.697s"
I1202 09:30:42.145920       1 shared_informer.go:318] Caches are synced for GC
I1202 09:30:42.145975       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1202 09:30:42.146004       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1202 09:30:42.149767       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fraud-659ff85d7" duration="189.911s"
I1202 09:30:42.152021       1 shared_informer.go:318] Caches are synced for ephemeral
I1202 09:30:42.153728       1 shared_informer.go:318] Caches are synced for PVC protection
I1202 09:30:42.153772       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1202 09:30:42.154161       1 shared_informer.go:318] Caches are synced for endpoint
I1202 09:30:42.234646       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1202 09:30:42.234671       1 shared_informer.go:318] Caches are synced for persistent volume
I1202 09:30:42.234646       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1202 09:30:42.235793       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1202 09:30:42.236562       1 shared_informer.go:318] Caches are synced for expand
I1202 09:30:42.239219       1 shared_informer.go:318] Caches are synced for attach detach
I1202 09:30:42.239300       1 shared_informer.go:318] Caches are synced for daemon sets
I1202 09:30:42.239332       1 shared_informer.go:318] Caches are synced for deployment
I1202 09:30:42.241065       1 shared_informer.go:318] Caches are synced for taint
I1202 09:30:42.241102       1 shared_informer.go:318] Caches are synced for stateful set
I1202 09:30:42.241150       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I1202 09:30:42.241278       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1202 09:30:42.241338       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1202 09:30:42.241366       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1202 09:30:42.241391       1 taint_manager.go:211] "Sending events to api server"
I1202 09:30:42.250500       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1202 09:30:42.250695       1 shared_informer.go:318] Caches are synced for resource quota
I1202 09:30:42.250808       1 shared_informer.go:318] Caches are synced for HPA
I1202 09:30:42.250876       1 shared_informer.go:318] Caches are synced for resource quota
I1202 09:30:42.334751       1 shared_informer.go:318] Caches are synced for disruption
I1202 09:30:42.539426       1 shared_informer.go:318] Caches are synced for garbage collector
I1202 09:30:42.539635       1 shared_informer.go:318] Caches are synced for garbage collector
I1202 09:30:42.539674       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1202 09:30:43.043929       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="898.579826ms"
I1202 09:30:43.044088       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="63.481s"
I1202 09:30:43.044161       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="900.288142ms"
I1202 09:30:43.044298       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="44.985s"
I1202 09:30:46.840667       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="105.240049ms"
I1202 09:30:46.841148       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="100.3s"
I1202 09:30:48.037588       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="98.535184ms"
I1202 09:30:48.037772       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/customer-7d78dfc6b" duration="94.119s"

* 
* ==> kube-proxy [9906483a64a8] <==
* I1202 09:30:32.639227       1 server_others.go:69] "Using iptables proxy"
I1202 09:30:32.680038       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1202 09:30:32.780838       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1202 09:30:32.786581       1 server_others.go:152] "Using iptables Proxier"
I1202 09:30:32.786719       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1202 09:30:32.786740       1 server_others.go:438] "Defaulting to no-op detect-local"
I1202 09:30:32.788046       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1202 09:30:32.788845       1 server.go:846] "Version info" version="v1.28.3"
I1202 09:30:32.788962       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1202 09:30:32.835799       1 config.go:97] "Starting endpoint slice config controller"
I1202 09:30:32.835865       1 config.go:188] "Starting service config controller"
I1202 09:30:32.835836       1 config.go:315] "Starting node config controller"
I1202 09:30:32.836770       1 shared_informer.go:311] Waiting for caches to sync for node config
I1202 09:30:32.836773       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1202 09:30:32.836798       1 shared_informer.go:311] Waiting for caches to sync for service config
I1202 09:30:32.938079       1 shared_informer.go:318] Caches are synced for service config
I1202 09:30:32.938149       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1202 09:30:32.938269       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [d47b6098246e] <==
* I1208 15:00:03.287540       1 server_others.go:69] "Using iptables proxy"
I1208 15:00:03.314925       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1208 15:00:03.394464       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1208 15:00:03.396312       1 server_others.go:152] "Using iptables Proxier"
I1208 15:00:03.396356       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1208 15:00:03.396363       1 server_others.go:438] "Defaulting to no-op detect-local"
I1208 15:00:03.397236       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1208 15:00:03.397623       1 server.go:846] "Version info" version="v1.28.3"
I1208 15:00:03.397657       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 15:00:03.399435       1 config.go:188] "Starting service config controller"
I1208 15:00:03.399461       1 config.go:315] "Starting node config controller"
I1208 15:00:03.399435       1 config.go:97] "Starting endpoint slice config controller"
I1208 15:00:03.400245       1 shared_informer.go:311] Waiting for caches to sync for service config
I1208 15:00:03.400258       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1208 15:00:03.400245       1 shared_informer.go:311] Waiting for caches to sync for node config
I1208 15:00:03.500729       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1208 15:00:03.500766       1 shared_informer.go:318] Caches are synced for node config
I1208 15:00:03.500786       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [d16a52046f23] <==
* I1208 14:59:58.810254       1 serving.go:348] Generated self-signed cert in-memory
W1208 15:00:00.687010       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1208 15:00:00.687631       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1208 15:00:00.687842       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1208 15:00:00.687992       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1208 15:00:00.788434       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1208 15:00:00.788467       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 15:00:00.790988       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1208 15:00:00.791859       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1208 15:00:00.792138       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1208 15:00:00.801183       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1208 15:00:00.801361       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [dac6984c06d4] <==
* I1202 09:30:22.856381       1 serving.go:348] Generated self-signed cert in-memory
W1202 09:30:26.535056       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1202 09:30:26.535148       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1202 09:30:26.535179       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1202 09:30:26.535201       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1202 09:30:26.654568       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1202 09:30:26.654627       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1202 09:30:26.658398       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1202 09:30:26.660025       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1202 09:30:26.660107       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1202 09:30:26.734649       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1202 09:30:26.835631       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1202 09:48:19.149222       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I1202 09:48:19.149341       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1202 09:48:19.149610       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1202 09:48:19.149963       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.779198    1702 topology_manager.go:215] "Topology Admit Handler" podUID="6b7439de-b844-4d30-aadc-dfc9e94fd31c" podNamespace="default" podName="rabbitmq-0"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.779260    1702 topology_manager.go:215] "Topology Admit Handler" podUID="e18e15f0-8d3d-42e3-8646-4dac3a789f6b" podNamespace="default" podName="customer-7d78dfc6b-gzg9h"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.779309    1702 topology_manager.go:215] "Topology Admit Handler" podUID="6b3d18dc-0083-434a-9191-55f586a2939b" podNamespace="default" podName="fraud-659ff85d7-dfxhz"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.779369    1702 topology_manager.go:215] "Topology Admit Handler" podUID="f7c05dc7-bba4-454e-b272-f37a751b1c70" podNamespace="default" podName="notification-6675954bc7-fm24n"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.793397    1702 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.793487    1702 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.795172    1702 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.798873    1702 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.878050    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/17d3e133-6df2-4903-ab37-e10a5c5d7167-tmp\") pod \"storage-provisioner\" (UID: \"17d3e133-6df2-4903-ab37-e10a5c5d7167\") " pod="kube-system/storage-provisioner"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.878402    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/972fe334-c183-409c-b342-d225e96ecd5b-lib-modules\") pod \"kube-proxy-h64xt\" (UID: \"972fe334-c183-409c-b342-d225e96ecd5b\") " pod="kube-system/kube-proxy-h64xt"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.878558    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/972fe334-c183-409c-b342-d225e96ecd5b-xtables-lock\") pod \"kube-proxy-h64xt\" (UID: \"972fe334-c183-409c-b342-d225e96ecd5b\") " pod="kube-system/kube-proxy-h64xt"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.892360    1702 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Dec 08 15:00:00 minikube kubelet[1702]: I1208 15:00:00.978896    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"postgres-pc-volume\" (UniqueName: \"kubernetes.io/host-path/1c065289-8945-41cb-8687-3fa869a1f3df-postgres-pc-volume\") pod \"postgres-0\" (UID: \"1c065289-8945-41cb-8687-3fa869a1f3df\") " pod="default/postgres-0"
Dec 08 15:00:02 minikube kubelet[1702]: I1208 15:00:02.475388    1702 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0e83a0b853e3250e8e7c441866fa00cca49c1afe63f6963d27be3beeca2bc469"
Dec 08 15:00:02 minikube kubelet[1702]: I1208 15:00:02.574391    1702 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="162affe57ae84a382b820d3cad982758e23df7ee9d799191df70b6295f421056"
Dec 08 15:00:02 minikube kubelet[1702]: I1208 15:00:02.584525    1702 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a5a4ee8e1a2f4c9598fbb307b67356ec900d7d10705779e9f27b40d9acc37bcd"
Dec 08 15:00:02 minikube kubelet[1702]: I1208 15:00:02.888466    1702 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1110558e4e217163c201f1c79ebd28b38e8c00849951434b74c4a63264057bb5"
Dec 08 15:00:02 minikube kubelet[1702]: I1208 15:00:02.972808    1702 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b4be2bc3cc3427d8476b23ecbb46fdd9d56f7dba5372573248f1513a369cca6f"
Dec 08 15:00:07 minikube kubelet[1702]: E1208 15:00:07.483371    1702 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 08 15:00:07 minikube kubelet[1702]: E1208 15:00:07.483423    1702 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 08 15:00:13 minikube kubelet[1702]: I1208 15:00:13.580854    1702 scope.go:117] "RemoveContainer" containerID="592126260faeb613c8b21a9ac1513065d709bf3ac282ddbafb6498f0babb0f89"
Dec 08 15:00:13 minikube kubelet[1702]: I1208 15:00:13.581155    1702 scope.go:117] "RemoveContainer" containerID="f48d1ee9abdfc8d3d0545e8eb8f91103685e8126768660e44ecb6046d9881794"
Dec 08 15:00:13 minikube kubelet[1702]: E1208 15:00:13.581424    1702 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(17d3e133-6df2-4903-ab37-e10a5c5d7167)\"" pod="kube-system/storage-provisioner" podUID="17d3e133-6df2-4903-ab37-e10a5c5d7167"
Dec 08 15:00:17 minikube kubelet[1702]: E1208 15:00:17.689855    1702 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 08 15:00:17 minikube kubelet[1702]: E1208 15:00:17.689903    1702 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 08 15:00:23 minikube kubelet[1702]: I1208 15:00:23.830776    1702 scope.go:117] "RemoveContainer" containerID="895bcab5178d121533a6d3709429c31b55802bac853d4a299d9accb98e9e13e5"
Dec 08 15:00:23 minikube kubelet[1702]: I1208 15:00:23.831293    1702 scope.go:117] "RemoveContainer" containerID="21962c721571920229b33758b1ca715de1d97a54202d2f7a19792ca75ef71390"
Dec 08 15:00:23 minikube kubelet[1702]: E1208 15:00:23.831497    1702 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fraud\" with CrashLoopBackOff: \"back-off 10s restarting failed container=fraud pod=fraud-659ff85d7-dfxhz_default(6b3d18dc-0083-434a-9191-55f586a2939b)\"" pod="default/fraud-659ff85d7-dfxhz" podUID="6b3d18dc-0083-434a-9191-55f586a2939b"
Dec 08 15:00:25 minikube kubelet[1702]: I1208 15:00:25.770397    1702 scope.go:117] "RemoveContainer" containerID="f48d1ee9abdfc8d3d0545e8eb8f91103685e8126768660e44ecb6046d9881794"
Dec 08 15:00:27 minikube kubelet[1702]: E1208 15:00:27.876401    1702 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 08 15:00:27 minikube kubelet[1702]: E1208 15:00:27.876452    1702 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 08 15:00:37 minikube kubelet[1702]: I1208 15:00:37.770780    1702 scope.go:117] "RemoveContainer" containerID="21962c721571920229b33758b1ca715de1d97a54202d2f7a19792ca75ef71390"
Dec 08 15:00:38 minikube kubelet[1702]: E1208 15:00:38.179409    1702 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 08 15:00:38 minikube kubelet[1702]: E1208 15:00:38.179488    1702 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 08 15:00:48 minikube kubelet[1702]: E1208 15:00:48.384381    1702 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 08 15:00:48 minikube kubelet[1702]: E1208 15:00:48.384445    1702 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 08 15:02:12 minikube kubelet[1702]: E1208 15:02:12.471405    1702 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="208a50eaa27b0bc7325915ea3806586e069841ba5542c5eacdd73fd6828c79f8" cmd=["rabbitmq-diagnostics","ping"]
Dec 08 15:04:56 minikube kubelet[1702]: W1208 15:04:56.806088    1702 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 08 15:09:56 minikube kubelet[1702]: W1208 15:09:56.803978    1702 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 08 15:14:56 minikube kubelet[1702]: W1208 15:14:56.801246    1702 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 08 15:18:37 minikube kubelet[1702]: I1208 15:18:37.899291    1702 topology_manager.go:215] "Topology Admit Handler" podUID="3d6280c8-0b23-46fb-88d5-dba44d611524" podNamespace="monitoring" podName="prometheus-5986749c4b-xk948"
Dec 08 15:18:38 minikube kubelet[1702]: I1208 15:18:38.074723    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3d6280c8-0b23-46fb-88d5-dba44d611524-config-volume\") pod \"prometheus-5986749c4b-xk948\" (UID: \"3d6280c8-0b23-46fb-88d5-dba44d611524\") " pod="monitoring/prometheus-5986749c4b-xk948"
Dec 08 15:18:38 minikube kubelet[1702]: I1208 15:18:38.074789    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-j2l2h\" (UniqueName: \"kubernetes.io/projected/3d6280c8-0b23-46fb-88d5-dba44d611524-kube-api-access-j2l2h\") pod \"prometheus-5986749c4b-xk948\" (UID: \"3d6280c8-0b23-46fb-88d5-dba44d611524\") " pod="monitoring/prometheus-5986749c4b-xk948"
Dec 08 15:19:36 minikube kubelet[1702]: I1208 15:19:36.309153    1702 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/prometheus-5986749c4b-xk948" podStartSLOduration=1.893310976 podCreationTimestamp="2024-12-08 15:18:37 +0000 UTC" firstStartedPulling="2024-12-08 15:18:38.479710933 +0000 UTC m=+1122.063467373" lastFinishedPulling="2024-12-08 15:19:35.894566947 +0000 UTC m=+1179.479233490" observedRunningTime="2024-12-08 15:19:36.308874295 +0000 UTC m=+1179.893540838" watchObservedRunningTime="2024-12-08 15:19:36.309077093 +0000 UTC m=+1179.893743636"
Dec 08 15:19:56 minikube kubelet[1702]: W1208 15:19:56.797798    1702 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 08 15:20:58 minikube kubelet[1702]: I1208 15:20:58.721673    1702 topology_manager.go:215] "Topology Admit Handler" podUID="100041c5-f22c-4fd1-b767-681639406869" podNamespace="monitoring" podName="prometheus-778579754b-sjwh7"
Dec 08 15:20:58 minikube kubelet[1702]: I1208 15:20:58.916413    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-79kjh\" (UniqueName: \"kubernetes.io/projected/100041c5-f22c-4fd1-b767-681639406869-kube-api-access-79kjh\") pod \"prometheus-778579754b-sjwh7\" (UID: \"100041c5-f22c-4fd1-b767-681639406869\") " pod="monitoring/prometheus-778579754b-sjwh7"
Dec 08 15:20:58 minikube kubelet[1702]: I1208 15:20:58.916473    1702 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/100041c5-f22c-4fd1-b767-681639406869-config-volume\") pod \"prometheus-778579754b-sjwh7\" (UID: \"100041c5-f22c-4fd1-b767-681639406869\") " pod="monitoring/prometheus-778579754b-sjwh7"
Dec 08 15:21:03 minikube kubelet[1702]: I1208 15:21:03.490861    1702 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/prometheus-778579754b-sjwh7" podStartSLOduration=2.354844383 podCreationTimestamp="2024-12-08 15:20:58 +0000 UTC" firstStartedPulling="2024-12-08 15:20:59.245924401 +0000 UTC m=+1262.831944590" lastFinishedPulling="2024-12-08 15:21:02.381906651 +0000 UTC m=+1265.967926840" observedRunningTime="2024-12-08 15:21:03.49058433 +0000 UTC m=+1267.076604519" watchObservedRunningTime="2024-12-08 15:21:03.490826633 +0000 UTC m=+1267.076846822"
Dec 08 15:21:03 minikube kubelet[1702]: I1208 15:21:03.943778    1702 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-j2l2h\" (UniqueName: \"kubernetes.io/projected/3d6280c8-0b23-46fb-88d5-dba44d611524-kube-api-access-j2l2h\") pod \"3d6280c8-0b23-46fb-88d5-dba44d611524\" (UID: \"3d6280c8-0b23-46fb-88d5-dba44d611524\") "
Dec 08 15:21:03 minikube kubelet[1702]: I1208 15:21:03.943846    1702 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3d6280c8-0b23-46fb-88d5-dba44d611524-config-volume\") pod \"3d6280c8-0b23-46fb-88d5-dba44d611524\" (UID: \"3d6280c8-0b23-46fb-88d5-dba44d611524\") "
Dec 08 15:21:03 minikube kubelet[1702]: I1208 15:21:03.944267    1702 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/3d6280c8-0b23-46fb-88d5-dba44d611524-config-volume" (OuterVolumeSpecName: "config-volume") pod "3d6280c8-0b23-46fb-88d5-dba44d611524" (UID: "3d6280c8-0b23-46fb-88d5-dba44d611524"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Dec 08 15:21:03 minikube kubelet[1702]: I1208 15:21:03.945917    1702 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3d6280c8-0b23-46fb-88d5-dba44d611524-kube-api-access-j2l2h" (OuterVolumeSpecName: "kube-api-access-j2l2h") pod "3d6280c8-0b23-46fb-88d5-dba44d611524" (UID: "3d6280c8-0b23-46fb-88d5-dba44d611524"). InnerVolumeSpecName "kube-api-access-j2l2h". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.044201    1702 reconciler_common.go:300] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3d6280c8-0b23-46fb-88d5-dba44d611524-config-volume\") on node \"minikube\" DevicePath \"\""
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.044253    1702 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-j2l2h\" (UniqueName: \"kubernetes.io/projected/3d6280c8-0b23-46fb-88d5-dba44d611524-kube-api-access-j2l2h\") on node \"minikube\" DevicePath \"\""
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.496461    1702 scope.go:117] "RemoveContainer" containerID="112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674"
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.514895    1702 scope.go:117] "RemoveContainer" containerID="112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674"
Dec 08 15:21:04 minikube kubelet[1702]: E1208 15:21:04.515851    1702 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674" containerID="112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674"
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.515907    1702 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674"} err="failed to get container status \"112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674\": rpc error: code = Unknown desc = Error response from daemon: No such container: 112de77609e758701a5849af2d3028d12b59d352cc8a35b8992508e5cbd03674"
Dec 08 15:21:04 minikube kubelet[1702]: I1208 15:21:04.768590    1702 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="3d6280c8-0b23-46fb-88d5-dba44d611524" path="/var/lib/kubelet/pods/3d6280c8-0b23-46fb-88d5-dba44d611524/volumes"

* 
* ==> storage-provisioner [dba2ab82ec36] <==
* I1208 15:00:25.916221       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1208 15:00:25.925922       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1208 15:00:25.926710       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1208 15:00:43.321366       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1208 15:00:43.321525       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"33890ff9-b3c5-4b08-812e-6fa87cec3ed2", APIVersion:"v1", ResourceVersion:"73443", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_db348a7f-720f-4d31-bceb-38790daa54e8 became leader
I1208 15:00:43.321585       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_db348a7f-720f-4d31-bceb-38790daa54e8!
I1208 15:00:43.422458       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_db348a7f-720f-4d31-bceb-38790daa54e8!

* 
* ==> storage-provisioner [f48d1ee9abdf] <==
* I1208 15:00:03.099479       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1208 15:00:13.113666       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

